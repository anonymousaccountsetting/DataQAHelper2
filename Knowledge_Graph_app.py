import streamlit as st
import rdflib
import os
import glob
import zipfile
import io
import json
from collections import defaultdict
from rdflib import Graph, URIRef, BNode, Literal,Namespace
from rdflib.namespace import RDF,RDFS, OWL
from urllib.parse import quote

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open('config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
            paths = config.get('kb_paths', {})
            return paths
    except FileNotFoundError:
        st.error("æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ config.jsonï¼")
        return {}
    except json.JSONDecodeError:
        st.error("é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼")
        return {}

# åŠ è½½é—®é¢˜
def load_questions_from_ttl(ttl_file_path):
    g = rdflib.Graph()
    try:
        # é¦–å…ˆå°è¯•è¯»å–æ–‡ä»¶å†…å®¹
        with open(ttl_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # æ£€æŸ¥å¹¶ä¿®æ­£å¸¸è§çš„è¯­æ³•é”™è¯¯
        content = content.replace('"\n', '" .\n')  # ç¡®ä¿æ¯ä¸ªè¯­å¥ä»¥ . ç»“å°¾
        content = content.replace('\r\n', '\n')    # ç»Ÿä¸€æ¢è¡Œç¬¦
        
        # ä¿®æ­£ç¼ºå°‘åˆ†å·çš„é—®é¢˜
        lines = content.split('\n')
        fixed_lines = []
        in_subject_block = False
        for i, line in enumerate(lines):
            line = line.strip()
            if not line or line.startswith('#'):
                fixed_lines.append(line)
                continue
                
            if line.endswith(' a owl:Ontology ;'):
                in_subject_block = True
                fixed_lines.append(line)
            elif in_subject_block:
                if line.startswith(':'):
                    if i < len(lines) - 1 and lines[i + 1].startswith(':'):
                        if not line.endswith(';'):
                            line = line + ' ;'
                    elif not line.endswith('.'):
                        line = line + ' .'
                    if line.endswith(' .'):
                        in_subject_block = False
                fixed_lines.append(line)
            else:
                fixed_lines.append(line)
        
        content = '\n'.join(fixed_lines)
        
        # # ä½¿ç”¨ä¿®æ­£åçš„å†…å®¹è§£æ
        # g.parse(data=content, format="turtle")
        g.parse(
            data=content,
            format="turtle",
            publicID="http://example.org/ontology#"  # æ·»åŠ åŸºç¡€ URI
        )
    except Exception as e:
        st.error(f"[è§£æé”™è¯¯] æ— æ³•è§£æ {ttl_file_path}: {str(e)}")
        return []

    # æ›´æ–°åçš„ SPARQL æŸ¥è¯¢
    query = """
    PREFIX : <http://example.org/ontology#>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT DISTINCT ?label ?tag ?optimDir ?overfitRisk
    WHERE {
      ?rule :hasQuestion ?qEntity .
      ?qEntity rdfs:label ?label .
      OPTIONAL { ?qEntity :hasTag ?tag. }
      OPTIONAL { ?qEntity :optimizationDirection ?optimDir. }
      OPTIONAL { ?qEntity :overfittingRisk ?overfitRisk. }
    }
    """

    question_list = []
    try:
        for row in g.query(query):
            q_label = str(row.label)
            q_tag = str(row.tag) if row.tag else None
            q_optim_dir = str(row.optimDir) if row.optimDir else None
            q_overfit = str(row.overfitRisk) if row.overfitRisk else None

            # å­˜æˆä¸€ä¸ªdict, ä¾¿äºåç»­ä½¿ç”¨
            question_list.append({
                "text": q_label,
                "tag": q_tag,
                "optim_dir": q_optim_dir,
                "overfitRisk": q_overfit
            })
    except Exception as e:
        st.warning(f"[SPARQLæŸ¥è¯¢é”™è¯¯] {ttl_file_path}: {e}")

    return question_list

# æ‰«æç›®å½•ä¸‹çš„TTLæ–‡ä»¶
def scan_ttl_files(model_type):
    """æ ¹æ®æ¨¡å‹ç±»å‹æ‰«æå¯¹åº”çŸ¥è¯†åº“ç›®å½•"""
    directory = st.session_state.kb_paths.get(model_type, "")
    if not directory:
        st.error(f"æœªé…ç½® {model_type} çŸ¥è¯†åº“è·¯å¾„ï¼")
        return []
    ttl_files = glob.glob(os.path.join(directory, "*.ttl"))
    return ttl_files

def add_new_rdf_file(file, model_type):
    """æ ¹æ®æ¨¡å‹ç±»å‹ä¿å­˜åˆ°å¯¹åº”ç›®å½•"""
    target_dir = st.session_state.kb_paths.get(model_type, "")
    if not target_dir:
        st.error("è¯·å…ˆé…ç½®çŸ¥è¯†åº“è·¯å¾„ï¼")
        return None
    os.makedirs(target_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨
    file_path = os.path.join(target_dir, file.name)
    with open(file_path, "wb") as f:
        f.write(file.getbuffer())
    return file_path

def delete_rdf_file(file_name, model_type):
    """æ ¹æ®æ¨¡å‹ç±»å‹åˆ é™¤æ–‡ä»¶"""
    target_dir = st.session_state.kb_paths.get(model_type, "")
    if not target_dir:
        return False
    file_path = os.path.join(target_dir, file_name)
    if os.path.exists(file_path):
        os.remove(file_path)
        return True
    return False

def collect_rdf_data(selected_questions, model_type, ttl_directory):
    """æ”¶é›†é€‰ä¸­é—®é¢˜ç›¸å…³çš„æ‰€æœ‰ RDF ä¸‰å…ƒç»„"""
    merged_graph = rdflib.Graph()
    label_counter = {}
    for item in selected_questions:
        file_path = os.path.join(ttl_directory, item["file"])
        g = rdflib.Graph()
        # g.parse(file_path, format="turtle")
        # ä¿®æ”¹åï¼ˆåœ¨ collect_rdf_data å‡½æ•°ä¸­ï¼‰
        g.parse(
            file_path,
            format="turtle",
            publicID="http://example.org/ontology#"  # æ·»åŠ åŸºç¡€ URI
        )
        # ä»æ–‡ä»¶åæå–æ¨¡å‹åç§°ï¼ˆå»æ‰.ttlæ‰©å±•åï¼‰
        model_name = os.path.splitext(item["file"])[0]  # å…³é”®ä¿®å¤
        # æŸ¥è¯¢é—®é¢˜å®ä½“åŠå…¶å…³è”è§„åˆ™
        query = f"""
            PREFIX : <http://example.org/ontology#>
            CONSTRUCT {{
                ?qEntity ?p ?o .
                ?rule ?rule_p ?rule_o .
            }}
            WHERE {{
                ?rule :hasQuestion ?qEntity ;
                      ?rule_p ?rule_o .
                ?qEntity rdfs:label ?label ;
                         ?p ?o .
                FILTER (str(?label) = "{item['question']}")
            }}
        """
        # === æ–°å¢ï¼šå¤„ç†é‡å¤æ ‡ç­¾ ===
        for s, p, o in g.query(query):
            # å¤„ç†é—®é¢˜å®ä½“çš„æ ‡ç­¾
            if p == RDFS.label and str(p) == "http://www.w3.org/2000/01/rdf-schema#label":
                original_label = str(o)
                # æ›´æ–°è®¡æ•°å™¨
                label_counter[original_label] = label_counter.get(original_label, 0) + 1
                count = label_counter[original_label]

                # å¦‚æœæ˜¯é‡å¤æ ‡ç­¾åˆ™æ·»åŠ åç¼€
                new_label = original_label if count == 1 else f"{original_label} [{count}]"

                # åˆ é™¤æ—§çš„ä¸‰å…ƒç»„
                merged_graph.remove((s, p, o))
                # æ·»åŠ æ–°æ ‡ç­¾ä¸‰å…ƒç»„
                merged_graph.add((s, p, Literal(new_label)))
            else:
                merged_graph.add((s, p, o))
    return merged_graph

# å®šä¹‰æ ¸å¿ƒå‘½åç©ºé—´
EX = Namespace("http://example.org/ontology#")
HYBRID = Namespace("http://example.org/hybrid/")

def generate_merged_ttl(merged_graph, model_names):
    # # æ¸…ç†æ¨¡å‹åç§°
    model_names_clean = [name.replace("Model", "").strip() for name in model_names]
    # ontology_uri = URIRef(f"http://example.org/{'_And_'.join(model_names_clean)}HybridModel")
    model_names_encoded = [quote(name.replace("Model", "").strip()) for name in model_names]
    ontology_uri = URIRef(f"http://example.org/merged/{'_and_'.join(model_names_encoded)}")
    # === å…³é”®ä¿®å¤ï¼šç»‘å®šæ‰€æœ‰å‘½åç©ºé—´å‰ç¼€ ===
    merged_graph.bind("ex", EX)
    merged_graph.bind("hybrid", HYBRID)
    merged_graph.bind("owl", OWL)
    merged_graph.bind("rdfs", RDFS)

    # æ·»åŠ æœ¬ä½“å£°æ˜
    merged_graph.add((ontology_uri, RDF.type, OWL.Ontology))
    merged_graph.add((ontology_uri, EX.modelType, Literal("Hybrid")))

    # æ·»åŠ æ ¸å¿ƒå±æ€§ï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼‰
    core_props = [
        (EX.lowerBound, "æ•°å€¼èŒƒå›´çš„ä¸‹ç•Œ"),
        (EX.upperBound, "æ•°å€¼èŒƒå›´çš„ä¸Šç•Œ"),
        (EX.hasTag, "é—®é¢˜å®ä½“çš„æ ‡ç­¾")
    ]
    for prop, comment in core_props:
        if (prop, RDF.type, RDF.Property) not in merged_graph:
            merged_graph.add((prop, RDF.type, RDF.Property))
            merged_graph.add((prop, RDFS.comment, Literal(comment)))

    # æ”¶é›†æ‰€æœ‰å”¯ä¸€æ ‡ç­¾
    tags = {str(o) for _, _, o in merged_graph.triples((None, EX.hasTag, None))}

    # ç”Ÿæˆè·¨æ¨¡å‹æ¯”è¾ƒè§„åˆ™
    for tag in tags:
        # ä½¿ç”¨å®Œæ•´çš„URIä»£æ›¿å‰ç¼€ï¼ˆé¿å…ä¾èµ–å‰ç¼€ç»‘å®šï¼‰
        cross_rule_uri = EX[f"CrossCompare_{tag}"]
        merged_graph.add((
            cross_rule_uri,
            RDF.type,
            EX.CrossModelComparisonRule
        ))
        merged_graph.add((
            cross_rule_uri,
            EX.comparedMetric,
            Literal(tag)
        ))
        # æ·»åŠ æ¨¡å‹åˆ—è¡¨
        for model in model_names_clean:
            merged_graph.add((
                cross_rule_uri,
                EX.comparedModels,
                Literal(model)
            ))
        # æ·»åŠ ä¼˜åŒ–æ–¹å‘
        optim_dir = "maximize" if "r2" in tag else "minimize"
        merged_graph.add((
            cross_rule_uri,
            EX.optimizationDirection,
            Literal(optim_dir)
        ))

    return merged_graph.serialize(format="turtle")

def map_rule_class_to_variable(rule_class_uri):
    rule_class = rdflib.URIRef(rule_class_uri)
    local_name = rule_class.split("#")[-1]
    if local_name.endswith("Rule"):
        variable = local_name[:-4].lower()
        return variable
    return "unknown_variable"

def get_model_required(g):
    """è·å–çŸ¥è¯†å›¾è°±çš„ modelRequired å±æ€§"""
    query = """
    PREFIX : <http://example.org/ontology#>
    SELECT ?modelRequired
    WHERE {
        ?s a owl:Ontology ;
           :modelRequired ?modelRequired .
    }
    """
    results = list(g.query(query))
    return str(results[0][0]) if results else None

def get_question_optimization_direction(g, question_text):
    """è·å–ç‰¹å®šé—®é¢˜çš„ä¼˜åŒ–æ–¹å‘"""
    query = f"""
    PREFIX : <http://example.org/ontology#>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT ?optimDir
    WHERE {{
        ?qEntity rdfs:label "{question_text}" ;
                 :optimizationDirection ?optimDir .
    }}
    """
    results = list(g.query(query))
    return str(results[0][0]) if results else None

# ç”Ÿæˆæ¨¡æ¿
def generate_template(question, model_rules, optim_dir="neutral", overfit_risk=None, has_conflicts=False):
    if not model_rules:
        return None, f"# {question}\n\nNo rules found for this question."

    template_lines = []
    template_lines.append(question)
    template_lines.append("")  # ç©ºè¡Œ

    # æ–°å¢ï¼šæ ‡å¿—å˜é‡ï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦ç”Ÿæˆè·¨æ¨¡å‹æ¯”è¾ƒä¸å»ºè®®
    has_r2_rules = False
    compare_data = []

    for model_rule in model_rules:
        model_name = model_rule["model_name"]
        model_display_name = model_rule["model_display_name"]
        rules = model_rule["rules"]

        first_rule = rules[0]
        rule_class = first_rule["rule_class"]
        variable = map_rule_class_to_variable(rule_class)

        unique_variable = f"{model_name}_{variable}"

        category = first_rule["rule_category"].lower().strip()

        if category == "constraint":
            placeholder = f"{{{{ {unique_variable} | round(3) }}}}"
            template_lines.append(
                f"The {variable.upper()} value of the {model_rule['model_display_name']} is {placeholder}.")
            template_lines.append("")

            sorted_rules = sorted([r for r in rules if r["lower_bound"] is not None],
                                  key=lambda r: float(r["lower_bound"]))

            for i, rule in enumerate(sorted_rules):
                lb = rule.get("lower_bound", 0.0)
                ub = rule.get("upper_bound", 0.0)
                description = rule["relationship_description"]
                recommendation = rule["recommendation"]

                condition = f"{unique_variable} >= {lb} and {unique_variable} < {ub}"

                if i == 0:
                    template_lines.append("{% if " + condition + " -%}")
                else:
                    template_lines.append("{% elif " + condition + " -%}")
                template_lines.append(f"The interpretation for {variable.upper()} is {description}. {recommendation}")

            last_rule = sorted_rules[-1]
            description = last_rule["relationship_description"]
            recommendation = last_rule["recommendation"]
            template_lines.append("{% else -%}")
            template_lines.append(f"The interpretation for {variable.upper()} is {description}. {recommendation}")
            template_lines.append("{% endif %}")
            template_lines.append("")  # ç©ºè¡Œ

        elif category == "descriptive":
            placeholder = f"{{{{ {unique_variable} }}}}"
            template_lines.append(f"The {variable.upper()} of the {model_rule['model_display_name']} is {placeholder}.")
            template_lines.append("")

            for rule in rules:
                description = rule["relationship_description"]
                recommendation = rule["recommendation"]

                if description.strip().lower().startswith(f"the {variable} is {{".lower()):
                    continue

                template_lines.append(f"{description}")
                template_lines.append("")

        else:
            placeholder = f"{{{{ {unique_variable} }}}}"
            template_lines.append(f"The {variable.upper()} value is {placeholder}.")
            template_lines.append("")

        compare_data.append({
            "var_name": unique_variable,
            "model_name": model_display_name
        })

    if len(compare_data) > 1:
        template_lines.append("\n{# --- Cross-model comparison --- #}")

        # ç”Ÿæˆæœ€ä½³å€¼åˆ¤æ–­
        if optim_dir == "maximize":
            template_lines.append("{% set best_value = [" + ", ".join(
                c['var_name'] for c in compare_data
            ) + "] | max %}")
            compare_operator = ">="
        elif optim_dir == "minimize":
            template_lines.append("{% set best_value = [" + ", ".join(
                c['var_name'] for c in compare_data
            ) + "] | min %}")
            compare_operator = "<="
        else:  # neutral
            template_lines.append("{% set best_value = None %}")
            compare_operator = "=="

        # ç”Ÿæˆæ¨¡å‹æ¯”è¾ƒæ¡ä»¶é“¾
        if optim_dir != "neutral":
            template_lines.append("\n")
            for i, model in enumerate(compare_data):
                condition = f"{model['var_name']} {compare_operator} best_value"
                if i == 0:
                    template_lines.append("{% if "+condition+" %}")
                else:
                    template_lines.append("{% elif "+condition+" %}")
                template_lines.append("The " + model["model_name"] + " has the best performance.")

            template_lines.append("{% endif %}\n")

        # æ·»åŠ è¿‡æ‹Ÿåˆæç¤º
        if overfit_risk:
            template_lines.append(f"\n{{# Overfitting Risk Note: #}}\n{overfit_risk}")

    # åœ¨æ¨¡æ¿æœ«å°¾æ·»åŠ å†²çªè­¦å‘Š
    if has_conflicts:
        template_lines.append("\n{# --- Conflict Warning --- #}")
        template_lines.append("Knowledge graph files have conflicting answer rules for the same question under the same model. A detailed examination is recommended.")

    file_name = f"answer_{variable}.txt"
    template_content = "\n".join(template_lines)
    return file_name, template_content

# æŸ¥è¯¢é—®é¢˜
def search_question(query_text, ttl_files):
    results = defaultdict(list)
    for ttl_file in ttl_files:
        questions = load_questions_from_ttl(ttl_file)  # ç°åœ¨è¿”å›å­—å…¸åˆ—è¡¨
        for q in questions:
            # åŒæ—¶åŒ¹é…æ–‡æœ¬å’Œæ ‡ç­¾
            if (query_text.lower() in q["text"].lower()) or \
               (query_text.lower() == q["tag"].lower()):
                results[ttl_file].append(q)  # è¿”å›æ•´ä¸ªå­—å…¸
    return results

# ä¿®æ”¹RDFæ–‡ä»¶çš„å‡½æ•°
def modify_rdf_file(model_type, file_name, question, new_description=None, new_recommendation=None, new_lower_bound=None,
                    new_upper_bound=None):
    """
    ä¿®æ”¹ RDF æ–‡ä»¶ä¸­çš„é—®é¢˜ã€ç­”æ¡ˆæè¿°ä»¥åŠä¸Šä¸‹è¾¹ç•Œã€‚
    """
    ttl_file_path = os.path.join(st.session_state.kb_paths[model_type], file_name)

    g = rdflib.Graph()
    try:
        g.parse(ttl_file_path, format="turtle")
    except Exception as e:
        st.error(f"[è§£æé”™è¯¯] æ— æ³•è§£æ {ttl_file_path}: {e}")
        return False

    # SPARQL æŸ¥è¯¢ï¼ŒæŸ¥æ‰¾ä¸é—®é¢˜ç›¸å…³çš„è§„åˆ™
    query = """
    PREFIX : <http://example.org/ontology#>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT ?rule ?desc ?rec ?lb ?ub
    WHERE {
        ?rule :hasQuestion ?qEntity ;
              :relationshipDescription ?desc ;
              :recommendation ?rec ;
              :lowerBound ?lb ;
              :upperBound ?ub .
        ?qEntity rdfs:label ?label .
        FILTER (str(?label) = "%s")
    }
    """ % question.replace('"', '\\"')  # Escape quotes in question

    # æ‰§è¡ŒæŸ¥è¯¢ï¼Œè·å–è§„åˆ™
    rules_to_modify = g.query(query)
    if not rules_to_modify:
        st.warning(f"æœªæ‰¾åˆ°ä¸é—®é¢˜ '{question}' ç›¸å…³çš„è§„åˆ™ã€‚")
        return False

    # æ›´æ–°æ¯ä¸ªåŒ¹é…çš„è§„åˆ™
    for row in rules_to_modify:
        rule = row[0]
        # å¦‚æœæä¾›äº†æ–°çš„æè¿°ï¼Œåˆ™ä¿®æ”¹æè¿°
        if new_description:
            g.set((rule, rdflib.URIRef("http://example.org/ontology#relationshipDescription"),
                   rdflib.Literal(new_description)))
        # å¦‚æœæä¾›äº†æ–°çš„æ¨èï¼Œåˆ™ä¿®æ”¹æ¨è
        if new_recommendation:
            g.set(
                (rule, rdflib.URIRef("http://example.org/ontology#recommendation"), rdflib.Literal(new_recommendation)))
        # å¦‚æœæä¾›äº†æ–°çš„ä¸Šä¸‹è¾¹ç•Œï¼Œåˆ™ä¿®æ”¹
        if new_lower_bound is not None:
            g.set((rule, rdflib.URIRef("http://example.org/ontology#lowerBound"), rdflib.Literal(new_lower_bound)))
        if new_upper_bound is not None:
            g.set((rule, rdflib.URIRef("http://example.org/ontology#upperBound"), rdflib.Literal(new_upper_bound)))

    # ä¿å­˜ä¿®æ”¹åçš„ RDF æ–‡ä»¶
    try:
        g.serialize(ttl_file_path, format="turtle")
        st.success(f"æ–‡ä»¶ {ttl_file_path} ä¿®æ”¹æˆåŠŸï¼")
        return True
    except Exception as e:
        st.error(f"ä¿®æ”¹æ–‡ä»¶ {ttl_file_path} å¤±è´¥: {e}")
        return False

def delete_rdf_file_question(ttl_file_path, question):
    """
    åˆ é™¤æŒ‡å®šé—®é¢˜åŠå…¶è§„åˆ™ã€‚
    """
    g = rdflib.Graph()
    try:
        g.parse(ttl_file_path, format="turtle")
    except Exception as e:
        st.error(f"[è§£æé”™è¯¯] æ— æ³•è§£æ {ttl_file_path}: {e}")
        return False

    query = """
    PREFIX : <http://example.org/ontology#>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    DELETE WHERE {
        ?rule :hasQuestion ?qEntity .
        ?qEntity rdfs:label ?label .
        FILTER (str(?label) = "%s")
    }
    """ % question.replace('"', '\\"')

    try:
        g.update(query)
        g.serialize(ttl_file_path, format="turtle")
        st.success(f"é—®é¢˜ '{question}' åŠç›¸å…³è§„åˆ™å·²åˆ é™¤ï¼")
        return True
    except Exception as e:
        st.error(f"åˆ é™¤é—®é¢˜ '{question}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def rename_question(g, old_question, new_question):
    # æ‰¾åˆ°ä¸ old_question åŒ¹é…çš„é—®é¢˜å®ä½“
    query = """
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT ?qEntity
    WHERE {
      ?qEntity rdfs:label ?label .
      FILTER (str(?label) = "%s")
    }
    """ % old_question.replace('"', '\\"')

    results = []
    for row in g.query(query):
        results.append(row[0])

    if not results:
        return False, "æœªæ‰¾åˆ°è¦é‡å‘½åçš„é—®é¢˜å®ä½“"

    qEntity = results[0]
    # åˆ é™¤æ—§çš„ label
    g.remove((qEntity, rdflib.RDFS.label, rdflib.Literal(old_question)))
    # æ·»åŠ æ–°çš„ label
    g.add((qEntity, rdflib.RDFS.label, rdflib.Literal(new_question)))
    return True, "é—®é¢˜æ–‡æœ¬å·²æ›´æ–°"

def update_question_in_file(ttl_file_path, old_question, new_question=None, new_tag=None, new_optim_dir=None,
                          new_overfit_risk=None, new_rule_category=None, new_rule_class=None,
                          new_rule_desc=None, new_rule_recom=None, new_bounds_list=None):
    """
    ä¿®æ”¹å·²æœ‰é—®é¢˜çš„æ‰€æœ‰å±æ€§ã€‚
    :param ttl_file_path: RDFæ–‡ä»¶è·¯å¾„
    :param old_question: åŸé—®é¢˜æ–‡æœ¬
    :param new_question: æ–°é—®é¢˜æ–‡æœ¬
    :param new_tag: æ–°æ ‡ç­¾
    :param new_optim_dir: æ–°ä¼˜åŒ–æ–¹å‘
    :param new_overfit_risk: æ–°è¿‡æ‹Ÿåˆé£é™©
    :param new_rule_category: æ–°è§„åˆ™ç±»åˆ«
    :param new_rule_class: æ–°è§„åˆ™ç±»
    :param new_rule_desc: æ–°è§„åˆ™æè¿°
    :param new_rule_recom: æ–°è§„åˆ™æ¨è
    :param new_bounds_list: æ–°çš„è¾¹ç•Œåˆ—è¡¨
    :return: (bool, msg)
    """
    g = rdflib.Graph()
    try:
        g.parse(ttl_file_path, format="turtle")
    except Exception as e:
        return False, f"è§£æé”™è¯¯: {e}"

    # æ‰¾åˆ°é—®é¢˜å®ä½“
    query = """
    PREFIX : <http://example.org/ontology#>
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT ?qEntity ?tag ?optimDir ?overfitRisk
    WHERE {
      ?qEntity rdfs:label ?label .
      OPTIONAL { ?qEntity :hasTag ?tag . }
      OPTIONAL { ?qEntity :optimizationDirection ?optimDir . }
      OPTIONAL { ?qEntity :overfittingRisk ?overfitRisk . }
      FILTER (str(?label) = "%s")
    }
    """ % old_question.replace('"', '\\"')

    results = list(g.query(query))
    if not results:
        return False, f"æœªæ‰¾åˆ°é—®é¢˜ '{old_question}'"

    q_entity = results[0][0]
    old_tag = results[0][1]
    old_optim_dir = results[0][2]
    old_overfit_risk = results[0][3]

    # æ›´æ–°é—®é¢˜æ–‡æœ¬
    if new_question:
        g.remove((q_entity, RDFS.label, Literal(old_question)))
        g.add((q_entity, RDFS.label, Literal(new_question)))

    # æ›´æ–°æ ‡ç­¾
    if new_tag is not None:
        if old_tag:
            g.remove((q_entity, EX.hasTag, old_tag))
        if new_tag:
            g.add((q_entity, EX.hasTag, Literal(new_tag)))

    # æ›´æ–°ä¼˜åŒ–æ–¹å‘
    if new_optim_dir is not None:
        if old_optim_dir:
            g.remove((q_entity, EX.optimizationDirection, old_optim_dir))
        if new_optim_dir:
            g.add((q_entity, EX.optimizationDirection, Literal(new_optim_dir)))

    # æ›´æ–°è¿‡æ‹Ÿåˆé£é™©
    if new_overfit_risk is not None:
        if old_overfit_risk:
            g.remove((q_entity, EX.overfittingRisk, old_overfit_risk))
        if new_overfit_risk:
            g.add((q_entity, EX.overfittingRisk, Literal(new_overfit_risk)))

    # æ›´æ–°è§„åˆ™
    if new_rule_category or new_rule_class or new_rule_desc or new_rule_recom or new_bounds_list:
        # æ‰¾åˆ°æ‰€æœ‰ç›¸å…³è§„åˆ™
        rule_query = """
        PREFIX : <http://example.org/ontology#>
        SELECT ?rule ?category ?class ?desc ?recom ?lb ?ub
        WHERE {
          ?rule :hasQuestion ?qEntity ;
                :ruleCategory ?category ;
                :ruleClass ?class ;
                :relationshipDescription ?desc ;
                :recommendation ?recom .
          OPTIONAL { ?rule :lowerBound ?lb . }
          OPTIONAL { ?rule :upperBound ?ub . }
          FILTER(?qEntity = <%s>)
        }
        """ % q_entity

        rules = list(g.query(rule_query))
        
        # åˆ é™¤æ‰€æœ‰æ—§è§„åˆ™
        for rule, _, _, _, _, _, _ in rules:
            g.remove((rule, None, None))
            g.remove((None, None, rule))

        # åˆ›å»ºæ–°è§„åˆ™
        if new_bounds_list:
            for bounds in new_bounds_list:
                new_rule = BNode()
                g.add((new_rule, RDF.type, EX[new_rule_class or "MSERule"]))
                g.add((new_rule, EX.hasQuestion, q_entity))
                g.add((new_rule, EX.ruleCategory, Literal(new_rule_category or "Descriptive")))
                g.add((new_rule, EX.ruleClass, EX[new_rule_class or "MSERule"]))
                if new_rule_desc:
                    g.add((new_rule, EX.relationshipDescription, Literal(new_rule_desc)))
                if new_rule_recom:
                    g.add((new_rule, EX.recommendation, Literal(new_rule_recom)))
                if bounds.get('lower_bound'):
                    g.add((new_rule, EX.lowerBound, Literal(float(bounds['lower_bound']))))
                if bounds.get('upper_bound'):
                    g.add((new_rule, EX.upperBound, Literal(float(bounds['upper_bound']))))
        else:
            # å¦‚æœæ²¡æœ‰æ–°çš„è¾¹ç•Œåˆ—è¡¨ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤è§„åˆ™
            new_rule = BNode()
            g.add((new_rule, RDF.type, EX[new_rule_class or "MSERule"]))
            g.add((new_rule, EX.hasQuestion, q_entity))
            g.add((new_rule, EX.ruleCategory, Literal(new_rule_category or "Descriptive")))
            g.add((new_rule, EX.ruleClass, EX[new_rule_class or "MSERule"]))
            if new_rule_desc:
                g.add((new_rule, EX.relationshipDescription, Literal(new_rule_desc)))
            if new_rule_recom:
                g.add((new_rule, EX.recommendation, Literal(new_rule_recom)))

    try:
        g.serialize(destination=ttl_file_path, format="turtle")
        return True, "é—®é¢˜åŠå…¶è§„åˆ™å·²æˆåŠŸæ›´æ–°"
    except Exception as e:
        return False, f"ä¿å­˜é”™è¯¯: {e}"

def delete_question(g, question_text):
    # æ‰¾åˆ°é—®é¢˜å®ä½“
    query_q = """
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT ?qEntity
    WHERE {
      ?qEntity rdfs:label ?label .
      FILTER (str(?label) = "%s")
    }
    """ % question_text.replace('"','\\"')

    results = list(g.query(query_q))
    if not results:
        return False, "æœªæ‰¾åˆ°éœ€è¦åˆ é™¤çš„é—®é¢˜"

    q_entity = results[0][0]
    # åˆ é™¤æ‰€æœ‰å¼•ç”¨q_entityçš„ä¸‰å…ƒç»„
    g.remove((q_entity, None, None))
    g.remove((None, None, q_entity))
    # è¿™æ ·å°±å½»åº•åˆ é™¤äº†è¿™ä¸ªé—®é¢˜å®ä½“
    return True, "é—®é¢˜å·²åˆ é™¤"

def add_question_to_file(ttl_file_path, q_text, tag, optim_dir, overfit_risk=None, rule_category=None, 
                        rule_class=None, rule_desc=None, rule_recom=None, bounds_list=None):
    # æ·»åŠ å‚æ•°æ ¡éªŒ
    if not all([q_text, tag, optim_dir]):
        return False, "é—®é¢˜æ–‡æœ¬ã€æ ‡ç­¾å’Œä¼˜åŒ–æ–¹å‘ä¸ºå¿…å¡«é¡¹"
    """
    åœ¨æŒ‡å®š RDF æ–‡ä»¶é‡Œï¼Œæ–°å¢ä¸€ä¸ªé—®é¢˜ä»¥åŠå¯¹åº”çš„ä¸€æ¡æˆ–å¤šæ¡ruleä¸‰å…ƒç»„ã€‚
    :param ttl_file_path: ç›®æ ‡RDFæ–‡ä»¶
    :param q_text: æ–°çš„é—®é¢˜æ–‡æœ¬
    :param tag: é—®é¢˜æ ‡ç­¾
    :param optim_dir: ä¼˜åŒ–æ–¹å‘
    :param overfit_risk: è¿‡æ‹Ÿåˆé£é™©
    :param rule_category: è§„åˆ™ç±»åˆ«ï¼ˆConstraint/Descriptiveï¼‰
    :param rule_class: è§„åˆ™ç±»å‹ï¼ˆå¦‚R2Ruleï¼‰
    :param rule_desc: è§„åˆ™æè¿°
    :param rule_recom: è§„åˆ™æ¨è
    :param bounds_list: è¾¹ç•Œåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«lower_boundå’Œupper_bound
    :return: (bool, msg)
    """
    g = rdflib.Graph()
    try:
        g.parse(ttl_file_path, format="turtle")
    except Exception as e:
        return False, f"è§£æé”™è¯¯: {e}"

    # 1) ç”Ÿæˆä¸€ä¸ªæ–°çš„questionå®ä½“
    question_entity = BNode()
    g.add((question_entity, rdflib.RDFS.label, Literal(q_text)))
    g.add((question_entity, EX.hasTag, Literal(tag)))
    g.add((question_entity, EX.optimizationDirection, Literal(optim_dir)))
    if overfit_risk:
        g.add((question_entity, EX.overfittingRisk, Literal(overfit_risk)))

    # 2) ç”Ÿæˆè§„åˆ™
    if rule_category == "Constraint":
        # ä¸ºæ¯ç»„è¾¹ç•Œåˆ›å»ºä¸€ä¸ªè§„åˆ™
        for i, bounds in enumerate(bounds_list):
            new_rule = BNode()
            g.add((new_rule, rdflib.RDF.type, URIRef(f"http://example.org/ontology#{rule_class}")))
            g.add((new_rule, EX.hasQuestion, question_entity))
            g.add((new_rule, EX.ruleCategory, Literal(rule_category)))
            
            # æ·»åŠ è¾¹ç•Œ
            if bounds.get('lower_bound') is not None:
                g.add((new_rule, EX.lowerBound, Literal(float(bounds['lower_bound']))))
            if bounds.get('upper_bound') is not None:
                g.add((new_rule, EX.upperBound, Literal(float(bounds['upper_bound']))))
            
            # æ·»åŠ æè¿°å’Œæ¨è
            if rule_desc:
                g.add((new_rule, EX.relationshipDescription, Literal(rule_desc)))
            if rule_recom:
                g.add((new_rule, EX.recommendation, Literal(rule_recom)))
    else:
        # å¯¹äºDescriptiveè§„åˆ™ï¼Œåªéœ€è¦ä¸€ä¸ªè§„åˆ™
        new_rule = BNode()
        g.add((new_rule, rdflib.RDF.type, URIRef(f"http://example.org/ontology#{rule_class}")))
        g.add((new_rule, EX.hasQuestion, question_entity))
        g.add((new_rule, EX.ruleCategory, Literal(rule_category)))
        
        if rule_desc:
            g.add((new_rule, EX.relationshipDescription, Literal(rule_desc)))
        if rule_recom:
            g.add((new_rule, EX.recommendation, Literal(rule_recom)))

    try:
        g.serialize(destination=ttl_file_path, format="turtle")
        return True, f"å·²æ·»åŠ æ–°é—®é¢˜'{q_text}'"
    except Exception as e:
        return False, f"ä¿å­˜é”™è¯¯: {e}"

def delete_question_in_file(ttl_file_path, question_text):
    """
    ä»RDFæ–‡ä»¶åˆ é™¤æŒ‡å®šé—®é¢˜ï¼ˆå«å…³è”ruleæˆ–ä»…åˆ é™¤questionå®ä½“? è§†éœ€æ±‚ï¼‰
    """
    g = rdflib.Graph()
    try:
        g.parse(ttl_file_path, format="turtle")
    except Exception as e:
        return False, f"è§£æé”™è¯¯: {e}"

    # æ‰¾åˆ° question entity
    query_q = """
    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
    SELECT ?qEntity
    WHERE {
      ?qEntity rdfs:label ?label .
      FILTER (str(?label) = "%s")
    }
    """ % question_text.replace('"','\\"')

    results = list(g.query(query_q))
    if not results:
        return False, f"æœªæ‰¾åˆ°é—®é¢˜ '{question_text}'"

    q_entity = results[0][0]

    # æ‰¾åˆ°æ‰€æœ‰ rule å…³è”åˆ°è¯¥q_entity
    query_rule = """
    PREFIX : <http://example.org/ontology#>
    SELECT ?rule
    WHERE {
      ?rule :hasQuestion ?qe .
      FILTER(?qe = <%s>)
    }
    """ % q_entity

    for rowr in g.query(query_rule):
        rule_uri = rowr[0]
        # åˆ é™¤ruleç›¸å…³ä¸‰å…ƒç»„
        g.remove((rule_uri, None, None))
        g.remove((None, None, rule_uri))

    # æœ€ååˆ é™¤q_entityè‡ªå·±
    g.remove((q_entity, None, None))
    g.remove((None, None, q_entity))

    try:
        g.serialize(destination=ttl_file_path, format="turtle")
        return True, f"å·²åˆ é™¤é—®é¢˜ '{question_text}' åŠå…¶ç›¸å…³rule"
    except Exception as e:
        return False, f"ä¿å­˜é”™è¯¯: {e}"

def map_question_to_variable(question_uri, g):
    """é€šè¿‡é—®é¢˜çš„ hasTag å±æ€§è·å–å˜é‡å"""
    query = f"""
        PREFIX : <http://example.org/ontology#>
        SELECT ?tag
        WHERE {{
            <{question_uri}> :hasTag ?tag .
        }}
    """
    result = g.query(query)
    return str(result.bindings[0]['tag']) if result else None

def truncate_filename(filename, max_length=20):
    """æˆªæ–­æ–‡ä»¶åï¼Œä¿ç•™æ‰©å±•å"""
    if len(filename) <= max_length:
        return filename
    name, ext = os.path.splitext(filename)
    return f"{name[:max_length-3]}...{ext}"

def build_kb_tree():
    """æ„å»ºçŸ¥è¯†å›¾è°±åº“çš„æ ‘çŠ¶ç»“æ„"""
    kb_tree = {}
    kb_types = {
        "regression": "å›å½’æ¨¡å‹çŸ¥è¯†åº“",
        "classification": "åˆ†ç±»æ¨¡å‹çŸ¥è¯†åº“",
        "math": "æ•°å­¦æ¨¡å‹çŸ¥è¯†åº“",
        "hybrid": "æ··åˆæ¨¡å‹çŸ¥è¯†åº“"
    }
    
    has_valid_path = False
    for kb_type, display_name in kb_types.items():
        path = st.session_state.kb_paths.get(kb_type, "")
        if path and os.path.exists(path):
            has_valid_path = True
            ttl_files = glob.glob(os.path.join(path, "*.ttl"))
            # å­˜å‚¨å®Œæ•´æ–‡ä»¶åå’Œæ˜¾ç¤ºç”¨çš„æˆªæ–­æ–‡ä»¶å
            kb_tree[display_name] = [
                {
                    "full_name": os.path.splitext(os.path.basename(f))[0],
                    "display_name": truncate_filename(os.path.splitext(os.path.basename(f))[0])
                }
                for f in ttl_files
            ]
    
    return kb_tree, has_valid_path

def compare_rules_boundaries(rules1, rules2):
    """
    æ¯”è¾ƒä¸¤ç»„è§„åˆ™çš„è¾¹ç•Œç»„æ•°ï¼Œè¿”å›è¾¹ç•Œç»„æ•°è¾ƒå¤šçš„è§„åˆ™ç»„
    :param rules1: ç¬¬ä¸€ç»„è§„åˆ™
    :param rules2: ç¬¬äºŒç»„è§„åˆ™
    :return: è¾¹ç•Œç»„æ•°è¾ƒå¤šçš„è§„åˆ™ç»„
    """
    # è®¡ç®—æ¯ç»„è§„åˆ™ä¸­çš„è¾¹ç•Œç»„æ•°
    def count_boundary_groups(rules):
        count = 0
        for rule in rules:
            if rule.get("lower_bound") is not None or rule.get("upper_bound") is not None:
                count += 1
        return count
    
    count1 = count_boundary_groups(rules1)
    count2 = count_boundary_groups(rules2)
    
    # è¿”å›è¾¹ç•Œç»„æ•°è¾ƒå¤šçš„è§„åˆ™ç»„
    if count1 >= count2:
        return rules1
    else:
        return rules2

# ä¸»å‡½æ•°
def main():
    st.set_page_config(page_title="RDF Models Viewer", layout="wide")
    
    # ä»é…ç½®æ–‡ä»¶åŠ è½½è·¯å¾„
    if "kb_paths" not in st.session_state:
        st.session_state.kb_paths = load_config()
    
    # åˆå§‹åŒ–é€‰ä¸­çš„æ–‡ä»¶å’Œå½“å‰é€‰ä¸­çš„çŸ¥è¯†åº“ç±»å‹
    if "selected_files" not in st.session_state:
        st.session_state.selected_files = []
    if "active_tab" not in st.session_state:
        st.session_state.active_tab = "ç”Ÿæˆæ¨¡æ¿"
    if "selected_kb_type" not in st.session_state:
        st.session_state.selected_kb_type = None
        
    # æ£€æŸ¥é…ç½®æ˜¯å¦æœ‰æ•ˆ
    if not all(st.session_state.kb_paths.values()):
        st.error("é…ç½®æ–‡ä»¶ä¸­çš„çŸ¥è¯†åº“è·¯å¾„é…ç½®ä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥ config.json æ–‡ä»¶ï¼")
        return

    # æ£€æŸ¥æ‰€æœ‰è·¯å¾„æ˜¯å¦å­˜åœ¨
    missing_paths = [path for path in st.session_state.kb_paths.values() if not os.path.exists(path)]
    if missing_paths:
        st.error(f"ä»¥ä¸‹è·¯å¾„ä¸å­˜åœ¨: {', '.join(missing_paths)}")
        return

    # åˆ›å»ºä¸‰åˆ—å¸ƒå±€
    left_col, middle_col, right_col = st.columns([1, 3, 1])

    with left_col:
        st.markdown("### çŸ¥è¯†å›¾è°±åº“")
        st.markdown("---")
        kb_tree, has_valid_path = build_kb_tree()
        if not has_valid_path:
            st.warning("æœªè¯»å–åˆ°çŸ¥è¯†å›¾è°±åº“")
        else:
            # æ¸…ç©ºä¹‹å‰çš„é€‰æ‹©
            if st.button("æ¸…é™¤é€‰æ‹©"):
                st.session_state.selected_files = []
                st.session_state.selected_kb_type = None
                st.rerun()
            
            for kb_type, files in kb_tree.items():
                with st.expander(f"ğŸ“ {kb_type}", expanded=True):
                    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–ç±»å‹è¢«é€‰ä¸­
                    is_other_type_selected = (st.session_state.selected_kb_type is not None and 
                                            st.session_state.selected_kb_type != kb_type)
                    
                    for file_info in files:
                        # åˆ›å»ºå”¯ä¸€çš„key
                        checkbox_key = f"select_{kb_type}_{file_info['full_name']}"
                        # æ£€æŸ¥æ˜¯å¦å·²é€‰ä¸­
                        is_selected = file_info['full_name'] in st.session_state.selected_files
                        
                        # ä½¿ç”¨checkboxè¿›è¡Œé€‰æ‹©ï¼Œå¦‚æœå…¶ä»–ç±»å‹è¢«é€‰ä¸­åˆ™ç¦ç”¨
                        checkbox = st.checkbox(
                            f"ğŸ“„ {file_info['display_name']}", 
                            key=checkbox_key,
                            value=is_selected,
                            help=file_info['full_name'],
                            disabled=is_other_type_selected
                        )
                        
                        if checkbox and not is_other_type_selected:
                            if file_info['full_name'] not in st.session_state.selected_files:
                                st.session_state.selected_files.append(file_info['full_name'])
                                st.session_state.selected_kb_type = kb_type
                                st.session_state.active_tab = "ç”Ÿæˆæ¨¡æ¿"
                                st.rerun()
                        elif not checkbox and file_info['full_name'] in st.session_state.selected_files:
                            st.session_state.selected_files.remove(file_info['full_name'])
                            if not st.session_state.selected_files:
                                st.session_state.selected_kb_type = None
                            st.rerun()
                    
                    # å¦‚æœå½“å‰ç±»å‹æœ‰é€‰ä¸­çš„æ–‡ä»¶ï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
                    if is_other_type_selected:
                        st.info(f"è¯·å…ˆå–æ¶ˆé€‰æ‹© {st.session_state.selected_kb_type} ä¸­çš„æ–‡ä»¶")

    with middle_col:
        st.title("RDF Models Viewer - Knowledge Base Configuration")
        # æ·»åŠ é¡¶éƒ¨èœå•ï¼ˆç§»é™¤æŸ¥è¯¢é—®é¢˜é€‰é¡¹ï¼‰
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ç”Ÿæˆæ¨¡æ¿", "å¢åŠ RDFæ–‡ä»¶", "åˆ é™¤RDFæ–‡ä»¶", "ä¿®æ”¹RDFæ–‡ä»¶", "ä¿®æ”¹é…ç½®"])

        with tab1:
            # æ˜¾ç¤ºçŸ¥è¯†åº“ç±»å‹é€‰æ‹©
            model_type = st.selectbox(
                "é€‰æ‹©çŸ¥è¯†åº“ç±»å‹",
                options=["regression", "classification", "math", "hybrid"],
                format_func=lambda x: {
                    "regression": "å›å½’æ¨¡å‹çŸ¥è¯†åº“",
                    "classification": "åˆ†ç±»æ¨¡å‹çŸ¥è¯†åº“",
                    "math": "æ•°å­¦æ¨¡å‹çŸ¥è¯†åº“",
                    "hybrid": "æ··åˆæ¨¡å‹çŸ¥è¯†åº“"
                }[x],
                key="gen_template_model_type"
            )

            # è·å–å¯¹åº”çŸ¥è¯†åº“è·¯å¾„
            ttl_directory = st.session_state.kb_paths.get(model_type, "")
            if not ttl_directory or not os.path.exists(ttl_directory):
                st.error(f"è·¯å¾„ '{ttl_directory}' æœªé…ç½®æˆ–ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥çŸ¥è¯†åº“é…ç½®ï¼")
                st.stop()

            # æ‰«æç›®å½•ä¸‹çš„TTLæ–‡ä»¶
            all_ttl_paths = scan_ttl_files(model_type)
            if not all_ttl_paths:
                st.error(f"åœ¨ç›®å½• {ttl_directory} ä¸­æœªæ‰¾åˆ°ä»»ä½• .ttl æ–‡ä»¶")
                st.stop()

            # æ„å»ºæ–‡ä»¶-é—®é¢˜å­—å…¸
            rdf_files_dict = {}
            for path in all_ttl_paths:
                file_name = os.path.basename(path)
                questions = load_questions_from_ttl(path)
                rdf_files_dict[file_name] = questions

            # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
            col1, col2 = st.columns([1, 2])
            
            with col1:
                # ä½¿ç”¨session_stateä¸­çš„é€‰ä¸­æ–‡ä»¶ä½œä¸ºé»˜è®¤å€¼
                selected_files = st.multiselect(
                    "é€‰æ‹©æ¨¡å‹", 
                    options=list(rdf_files_dict.keys()), 
                    default=[f"{f}.ttl" for f in st.session_state.selected_files if f in [os.path.splitext(k)[0] for k in rdf_files_dict.keys()]]
                )
                st.markdown("---")

                selected_questions = []
                for file_name in selected_files:
                    questions = rdf_files_dict[file_name]
                    if questions:
                        st.subheader(file_name)
                        for q_dict in questions:  # q_dict æ˜¯åŒ…å« text å’Œ tag çš„å­—å…¸
                            question_text = q_dict["text"]
                            checkbox_id = f"{file_name}_{question_text}"
                            if st.checkbox(question_text, key=checkbox_id):
                                selected_questions.append({"file": file_name, "question": question_text})

                generate = st.button("ç”Ÿæˆé—®ç­”æ¨¡æ¿")

            with col2:
                if selected_files:
                    st.write("### å·²é€‰æ‹©çš„æ¨¡å‹åŠå…¶é—®é¢˜")
                    for file_name in selected_files:
                        st.write(f"**{file_name}**")
                        questions = rdf_files_dict[file_name]
                        for q_dict in questions:
                            question_text = q_dict["text"]
                            if {"file": file_name, "question": question_text} in selected_questions:
                                st.markdown(f"- [x] {question_text}")
                            else:
                                st.markdown(f"- [ ] {question_text}")
                else:
                    st.write("### è¯·åœ¨å·¦ä¾§é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å‹ï¼Œå¹¶é€‰æ‹©ç›¸åº”çš„é—®é¢˜ã€‚")

                if generate:
                    if not selected_questions:
                        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªé—®é¢˜ä»¥ç”Ÿæˆé—®ç­”æ¨¡æ¿ã€‚")
                    else:
                        st.write("### ç”Ÿæˆä¸­...")
                        grouped_questions = defaultdict(list)
                        for item in selected_questions:
                            grouped_questions[item["question"]].append(item["file"])

                        zip_buffer = io.BytesIO()
                        with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zip_file:
                            for question, files in grouped_questions.items():
                                model_rules = []
                                optim_dir = "neutral"  # é»˜è®¤å€¼
                                overfit_risk = None
                                has_conflicts = False

                                # æ£€æŸ¥æ¨¡å‹è¦æ±‚å’Œä¼˜åŒ–æ–¹å‘
                                if len(files) > 1:
                                    model_requirements = {}
                                    optimization_directions = {}
                                    
                                    for file_name in files:
                                        # ç¡®ä¿æ–‡ä»¶ååŒ…å«.ttlæ‰©å±•å
                                        if not file_name.endswith('.ttl'):
                                            file_name = f"{file_name}.ttl"
                                        ttl_path = os.path.join(ttl_directory, file_name)
                                        g = rdflib.Graph()
                                        try:
                                            g.parse(ttl_path, format="turtle")
                                            # è·å–æ¨¡å‹è¦æ±‚
                                            model_req = get_model_required(g)
                                            if model_req:
                                                model_requirements[file_name] = model_req
                                            
                                            # è·å–ä¼˜åŒ–æ–¹å‘
                                            optim = get_question_optimization_direction(g, question)
                                            if optim:
                                                optimization_directions[file_name] = optim
                                        except Exception as e:
                                            st.error(f"[è§£æé”™è¯¯] æ— æ³•è§£æ {file_name}: {e}")
                                            continue

                                    # æ£€æŸ¥æ¨¡å‹è¦æ±‚æ˜¯å¦ç›¸åŒ
                                    unique_model_reqs = set(model_requirements.values())
                                    if len(unique_model_reqs) == 1:
                                        # å¦‚æœæ¨¡å‹è¦æ±‚ç›¸åŒï¼Œæ£€æŸ¥ä¼˜åŒ–æ–¹å‘
                                        unique_optim_dirs = set(optimization_directions.values())
                                        if len(unique_optim_dirs) > 1:
                                            has_conflicts = True
                                        else:
                                            # å¦‚æœä¼˜åŒ–æ–¹å‘ä¹Ÿç›¸åŒï¼Œæ¯”è¾ƒè§„åˆ™è¾¹ç•Œç»„æ•°
                                            if len(files) == 2:
                                                rules1 = model_rules[0]["rules"]
                                                rules2 = model_rules[1]["rules"]
                                                # ä¿ç•™è¾¹ç•Œç»„æ•°è¾ƒå¤šçš„è§„åˆ™
                                                model_rules[0]["rules"] = compare_rules_boundaries(rules1, rules2)
                                                model_rules[1]["rules"] = model_rules[0]["rules"]

                                # ä»ç¬¬ä¸€ä¸ªæ–‡ä»¶è·å–ä¼˜åŒ–æ–¹å‘å’Œè¿‡æ‹Ÿåˆé£é™©
                                if files:
                                    first_file = files[0]
                                    # ç¡®ä¿æ–‡ä»¶ååŒ…å«.ttlæ‰©å±•å
                                    if not first_file.endswith('.ttl'):
                                        first_file = f"{first_file}.ttl"
                                    ttl_path = os.path.join(ttl_directory, first_file)
                                    g = rdflib.Graph()
                                    g.parse(ttl_path, format="turtle")

                                    # æŸ¥è¯¢è¯¥é—®é¢˜çš„ä¼˜åŒ–å±æ€§
                                    query = f"""
                                        PREFIX : <http://example.org/ontology#>
                                        SELECT ?optimDir ?overfitRisk
                                        WHERE {{
                                            ?qEntity rdfs:label "{question}" ;
                                                     :optimizationDirection ?optimDir ;
                                                     :overfittingRisk ?overfitRisk .
                                        }}
                                    """
                                    result = g.query(query)
                                    if result:
                                        row = result.bindings[0]
                                        optim_dir = str(row.get('optimDir', 'neutral'))
                                        overfit_risk = str(row.get('overfitRisk', ''))
                                
                                for file_name in files:
                                    # ç¡®ä¿æ–‡ä»¶ååŒ…å«.ttlæ‰©å±•å
                                    if not file_name.endswith('.ttl'):
                                        file_name = f"{file_name}.ttl"
                                    ttl_path = os.path.join(ttl_directory, file_name)
                                    g = rdflib.Graph()
                                    try:
                                        g.parse(ttl_path, format="turtle")
                                    except Exception as e:
                                        st.error(f"[è§£æé”™è¯¯] æ— æ³•è§£æ {file_name}: {e}")
                                        continue
                                    query = """
                                    PREFIX : <http://example.org/ontology#>
                                    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
                                    SELECT ?ruleClass ?lowerBound ?upperBound ?relationshipDescription ?recommendation ?ruleCategory
                                    WHERE {
                                      ?rule a ?ruleClass ;
                                            :hasQuestion ?qEntity ;
                                            :relationshipDescription ?relationshipDescription ;
                                            :recommendation ?recommendation ;
                                            :ruleCategory ?ruleCategory .
                                      OPTIONAL { ?rule :lowerBound ?lowerBound . }
                                      OPTIONAL { ?rule :upperBound ?upperBound . }
                                      ?qEntity rdfs:label ?label ;
                                                   :hasTag ?tag .
                                      FILTER (str(?label) = "%s")
                                    }
                                    """ % question.replace('"', '\\"')
                                    rules = []

                                    try:
                                        for row in g.query(query):
                                            rule_class = str(row.ruleClass).split("#")[-1]
                                            lower_bound = row.lowerBound.toPython() if row.lowerBound else None
                                            upper_bound = row.upperBound.toPython() if row.upperBound else None
                                            relationship_description = str(row.relationshipDescription)
                                            recommendation = str(row.recommendation)
                                            rule_category = str(row.ruleCategory).strip('"') if row.ruleCategory else "none"

                                            rules.append({
                                                "rule_class": rule_class,
                                                "lower_bound": lower_bound,
                                                "upper_bound": upper_bound,
                                                "relationship_description": relationship_description,
                                                "recommendation": recommendation,
                                                "rule_category": rule_category
                                            })
                                    except Exception as e:
                                        st.warning(f"[SPARQLæŸ¥è¯¢é”™è¯¯] {file_name}: {e}")

                                    if rules:
                                        # å»æ‰æ‰©å±•åï¼Œå¦‚ .ttl
                                        base_no_ext, _ = os.path.splitext(file_name)
                                        # è¿›ä¸€æ­¥æ›¿æ¢ Model => model, Regression => Regression
                                        model_display_name = base_no_ext.replace('Model', ' model').replace('Regression',
                                                                                                            ' regression')
                                        # model_name = model_display_name.lower().replace(' ', '_')
                                        model_name = base_no_ext
                                        model_rules.append({
                                            "model_name": model_name,
                                            "model_display_name": model_display_name,
                                            "rules": rules
                                        })

                                if model_rules:
                                    # æ£€æŸ¥æ¨¡å‹è¦æ±‚å’Œä¼˜åŒ–æ–¹å‘
                                    if len(model_rules) > 1:
                                        model_requirements = {}
                                        optimization_directions = {}
                                        
                                        for model_rule in model_rules:
                                            file_name = model_rule["model_name"]
                                            # ç¡®ä¿æ–‡ä»¶ååŒ…å«.ttlæ‰©å±•å
                                            if not file_name.endswith('.ttl'):
                                                file_name = f"{file_name}.ttl"
                                            ttl_path = os.path.join(ttl_directory, file_name)
                                            g = rdflib.Graph()
                                            try:
                                                g.parse(ttl_path, format="turtle")
                                                # è·å–æ¨¡å‹è¦æ±‚
                                                model_req = get_model_required(g)
                                                if model_req:
                                                    model_requirements[file_name] = model_req
                                                
                                                # è·å–ä¼˜åŒ–æ–¹å‘
                                                optim = get_question_optimization_direction(g, question)
                                                if optim:
                                                    optimization_directions[file_name] = optim
                                            except Exception as e:
                                                st.error(f"[è§£æé”™è¯¯] æ— æ³•è§£æ {file_name}: {e}")
                                                continue

                                        # æ£€æŸ¥æ¨¡å‹è¦æ±‚æ˜¯å¦ç›¸åŒ
                                        unique_model_reqs = set(model_requirements.values())
                                        if len(unique_model_reqs) == 1:
                                            # å¦‚æœæ¨¡å‹è¦æ±‚ç›¸åŒï¼Œæ£€æŸ¥ä¼˜åŒ–æ–¹å‘
                                            unique_optim_dirs = set(optimization_directions.values())
                                            if len(unique_optim_dirs) > 1:
                                                has_conflicts = True
                                            else:
                                                # å¦‚æœä¼˜åŒ–æ–¹å‘ä¹Ÿç›¸åŒï¼Œæ¯”è¾ƒè§„åˆ™è¾¹ç•Œç»„æ•°
                                                if len(model_rules) == 2:
                                                    rules1 = model_rules[0]["rules"]
                                                    rules2 = model_rules[1]["rules"]
                                                    # ä¿ç•™è¾¹ç•Œç»„æ•°è¾ƒå¤šçš„è§„åˆ™
                                                    model_rules[0]["rules"] = compare_rules_boundaries(rules1, rules2)
                                                    model_rules[1]["rules"] = model_rules[0]["rules"]

                                    file_out_name, template_content = generate_template(
                                        question,
                                        model_rules,
                                        optim_dir=optim_dir,
                                        overfit_risk=overfit_risk,
                                        has_conflicts=has_conflicts
                                    )
                                    if not file_out_name:
                                        st.warning(f"æœªç”Ÿæˆæ¨¡æ¿æ–‡ä»¶ï¼Œå› ä¸ºæœªæ‰¾åˆ°è§„åˆ™ã€‚é—®é¢˜ï¼š{question}")
                                        continue

                                    zip_file.writestr(file_out_name, template_content)

                            # æ”¶é›†è·¨æ¨¡å‹æ•°æ®
                            # merged_graph = collect_rdf_data(selected_questions, model_type, ttl_directory)
                            # ä¿®æ”¹åï¼ˆç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„çŸ¥è¯†åº“è·¯å¾„ï¼‰
                            kb_type_map = {
                                "regression": st.session_state.kb_paths["regression"],
                                "classification": st.session_state.kb_paths["classification"],
                                "math": st.session_state.kb_paths["math"],
                                "hybrid": st.session_state.kb_paths["hybrid"]
                            }
                            ttl_directory = kb_type_map[model_type]
                            merged_graph = collect_rdf_data(selected_questions, model_type,ttl_directory)
                            merged_ttl = generate_merged_ttl(merged_graph, [os.path.splitext(f)[0] for f in selected_files])

                            # å†™å…¥æ•´åˆå›¾è°±
                            model_names_clean = [os.path.splitext(f)[0] for f in selected_files]  # ç§»é™¤.ttlæ‰©å±•å
                            filename = f"Integrated_{'_And_'.join(model_names_clean)}.ttl"
                            zip_file.writestr(filename, merged_ttl)
                            # ç”Ÿæˆè„šæœ¬
                            script_lines = []
                            script_lines.append("import pandas as pd")
                            script_lines.append("import DataScienceComponents as DC")
                            script_lines.append("import NLGComponents as NC")
                            script_lines.append("from dash import html")
                            script_lines.append("from jinja2 import Environment, BaseLoader")
                            # è„šæœ¬ä¸»ä½“

                            script_lines.append("# åŠ è½½æ•°æ®ï¼ˆä¾‹å¦‚ä»CSVæ–‡ä»¶ï¼‰")
                            script_lines.append("data = pd.read_csv('some_data.csv')")
                            script_lines.append("Xcol = ['col1', 'col2',...]")
                            script_lines.append("ycol = 'target'")
                            # åœ¨é€‰æ‹©é—®é¢˜å’Œæ¨¡å‹åï¼Œåˆå§‹åŒ– variables_to_fill å­—å…¸
                            variables_to_fill = {}
                            question_to_variable_map = {
                                "Is the relationship between the variables strong?": "r2",
                                "Is the MAPE value acceptable?": "mape",
                                "What is the MAE of this model?": "mae",
                                "What is the RMSE of this model?": "rmse",
                                "What is the MSE of this model?": "mse"
                            }

                            # è®°å½•å·²è°ƒç”¨çš„æ¨¡å‹
                            called_models = set()  # ç”¨äºè¿½è¸ªå·²ç»è°ƒç”¨è¿‡çš„æ¨¡å‹
                            # æ ¹æ®é€‰æ‹©çš„é—®é¢˜å’Œæ¨¡å‹ï¼Œè°ƒç”¨æ•°æ®ç§‘å­¦ç»„ä»¶è¿›è¡Œæ¨¡å‹æ‹Ÿåˆ
                            for item in selected_questions:
                                file_name = item["file"]
                                question = item["question"]
                                # æ ¹æ®é€‰æ‹©çš„é—®é¢˜è·å–å¯¹åº”çš„å˜é‡
                                var = question_to_variable_map.get(question)
                                # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ¨¡å‹ç±»å‹ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„ä»£ç 
                                if file_name == "LinearRegressionModel.ttl":
                                    # å¦‚æœæ²¡æœ‰è°ƒç”¨è¿‡çº¿æ€§å›å½’æ¨¡å‹çš„æ‹Ÿåˆå‡½æ•°ï¼Œåˆ™è°ƒç”¨
                                    if "linear_regression" not in called_models:
                                        script_lines.append(
                                            "linear_model_results = DC.ModelFitting().LinearSKDefaultModel(data, Xcol, ycol)")
                                        called_models.add("linear_regression")  # æ ‡è®°çº¿æ€§å›å½’æ¨¡å‹å·²è°ƒç”¨

                                elif file_name == "GradientBoostingRegressionModel.ttl":
                                    # å¦‚æœæ²¡æœ‰è°ƒç”¨è¿‡GBæ¨¡å‹çš„æ‹Ÿåˆå‡½æ•°ï¼Œåˆ™è°ƒç”¨
                                    if "gradient_boosting" not in called_models:
                                        script_lines.append(
                                            "gradient_boosting_results = DC.ModelFitting().GradientBoostingDefaultModel(data, Xcol, ycol)")
                                        called_models.add("gradient_boosting")  # æ ‡è®°GBæ¨¡å‹å·²è°ƒç”¨

                            script_lines.append("env = Environment(loader=BaseLoader())")
                            script_lines.append("app, listTabs = NC.start_app()")
                            script_lines.append("QA = template_name.render(variable=variable)")
                            script_lines.append("children = [html.P(QA)]")
                            script_lines.append("NC.dash_tab_add(listTabs, label, children)")

                            script_lines.append("NC.run_app(app, listTabs, portnum=8050)")

                            full_script = "\n".join(script_lines)
                            zip_file.writestr("auto_generated_pipeline.py", full_script)

                        zip_buffer.seek(0)
                        st.success("é—®ç­”æ¨¡æ¿ç”ŸæˆæˆåŠŸï¼")
                        st.download_button(label="ä¸‹è½½é—®ç­”æ¨¡æ¿ ZIP", data=zip_buffer, file_name="qa_templates.zip",
                                           mime="application/zip")

        with tab2:
            model_type = st.selectbox("é€‰æ‹©ç›®æ ‡çŸ¥è¯†åº“ç±»å‹", ["regression", "classification", "math", "hybrid"])
            uploaded_file = st.file_uploader("ä¸Šä¼ æ–°çš„RDFæ–‡ä»¶", type=["ttl"])
            if uploaded_file is not None:
                file_path = add_new_rdf_file(uploaded_file, model_type)
                st.success(f"æ–‡ä»¶ {uploaded_file.name} ä¸Šä¼ æˆåŠŸï¼Œä¿å­˜åœ¨ {file_path}")

        with tab3:
            model_type = st.selectbox("é€‰æ‹©çŸ¥è¯†åº“ç±»å‹", ["regression", "classification", "math", "hybrid"])
            ttl_files = scan_ttl_files(model_type)
            file_to_delete = st.selectbox("é€‰æ‹©è¦åˆ é™¤çš„æ–‡ä»¶", [os.path.basename(f) for f in ttl_files])
            if st.button("åˆ é™¤"):
                success = delete_rdf_file(file_to_delete, model_type)
                if success:
                    st.success(f"æ–‡ä»¶ {file_to_delete} å·²æˆåŠŸåˆ é™¤ï¼")
                else:
                    st.error(f"æœªèƒ½æ‰¾åˆ°æ–‡ä»¶ {file_to_delete}ï¼Œåˆ é™¤å¤±è´¥ï¼")

        with tab4:
            # é€‰æ‹©çŸ¥è¯†åº“ç±»å‹
            model_type = st.selectbox(
                "é€‰æ‹©çŸ¥è¯†åº“ç±»å‹",
                options=["regression", "classification", "math", "hybrid"],
                format_func=lambda x: {
                    "regression": "å›å½’æ¨¡å‹çŸ¥è¯†åº“",
                    "classification": "åˆ†ç±»æ¨¡å‹çŸ¥è¯†åº“",
                    "math": "æ•°å­¦æ¨¡å‹çŸ¥è¯†åº“",
                    "hybrid": "æ··åˆæ¨¡å‹çŸ¥è¯†åº“"
                }[x]
            )
            ttl_directory = st.session_state.kb_paths.get(model_type, "")
            if not ttl_directory:
                st.error(f"æœªé…ç½® {model_type} è·¯å¾„ï¼")
                st.stop()

            ttl_files = scan_ttl_files(model_type)
            ttl_files_names = [os.path.basename(f) for f in ttl_files]

            # é€‰æ‹©è¦ä¿®æ”¹çš„æ–‡ä»¶
            selected_file = st.selectbox("é€‰æ‹©è¦ä¿®æ”¹çš„RDFæ–‡ä»¶", ["è¯·é€‰æ‹©æ–‡ä»¶"] + ttl_files_names)
            if selected_file == "è¯·é€‰æ‹©æ–‡ä»¶":
                st.warning("è¯·å…ˆé€‰æ‹©è¦ä¿®æ”¹çš„RDFæ–‡ä»¶ã€‚")
            else:
                # å·²é€‰æ‹©æ–‡ä»¶
                file_path = os.path.join(ttl_directory, selected_file)

                st.write(f"### æ‚¨é€‰æ‹©çš„æ–‡ä»¶ï¼š{selected_file}")

                # è¯»å–è¯¥æ–‡ä»¶å†…çš„é—®é¢˜
                questions_in_file = load_questions_from_ttl(file_path)

                # æ„å»ºä¸‹æ‹‰åˆ—è¡¨, æ·»åŠ ä¸€ä¸ª"æ·»åŠ æ–°é—®é¢˜"é€‰é¡¹
                choice_list = ["æ·»åŠ æ–°é—®é¢˜"] + questions_in_file
                selected_question = st.selectbox("è¯·é€‰æ‹©è¦æ“ä½œçš„é—®é¢˜ï¼Œæˆ–æ·»åŠ æ–°é—®é¢˜", choice_list)

                if selected_question == "æ·»åŠ æ–°é—®é¢˜":
                    # ç”¨æˆ·æƒ³æ·»åŠ æ–°é—®é¢˜
                    st.write("#### æ·»åŠ æ–°é—®é¢˜")

                    # åŸºæœ¬ä¿¡æ¯éƒ¨åˆ†
                    st.write("##### åŸºæœ¬ä¿¡æ¯")
                    new_q_text = st.text_input("æ–°é—®é¢˜æ–‡æœ¬")
                    new_tag = st.text_input("é—®é¢˜æ ‡ç­¾")
                    new_optim_dir = st.selectbox("ä¼˜åŒ–æ–¹å‘", ["maximize", "minimize", "neutral"])
                    new_overfit_risk = st.text_area("è¿‡æ‹Ÿåˆé£é™©è¯´æ˜(å¯é€‰)")

                    # è§„åˆ™ä¿¡æ¯éƒ¨åˆ†
                    st.write("##### è§„åˆ™ä¿¡æ¯")
                    new_rule_category = st.selectbox("è§„åˆ™ç±»åˆ«", ["Constraint", "Descriptive"])
                    new_rule_class = st.text_input("è§„åˆ™ç±»å‹(å¦‚R2Rule)")
                    new_rule_desc = st.text_area("è§„åˆ™æè¿°")
                    new_rule_recom = st.text_area("è§„åˆ™æ¨è")

                    # è¾¹ç•Œä¿¡æ¯éƒ¨åˆ†
                    st.write("##### è¾¹ç•Œä¿¡æ¯")
                    if "bounds_list" not in st.session_state:
                        st.session_state.bounds_list = [{"lower_bound": "", "upper_bound": ""}]

                    for i, bounds in enumerate(st.session_state.bounds_list):
                        col1, col2, col3 = st.columns([2, 2, 1])
                        with col1:
                            bounds["lower_bound"] = st.text_input(f"ä¸‹ç•Œ {i+1}", bounds["lower_bound"])
                        with col2:
                            bounds["upper_bound"] = st.text_input(f"ä¸Šç•Œ {i+1}", bounds["upper_bound"])
                        with col3:
                            if st.button("åˆ é™¤", key=f"del_{i}"):
                                st.session_state.bounds_list.pop(i)
                                st.rerun()

                    if st.button("æ·»åŠ æ–°çš„è¾¹ç•Œç»„"):
                        st.session_state.bounds_list.append({"lower_bound": "", "upper_bound": ""})
                        st.rerun()

                    if st.button("ç¡®è®¤ä¿®æ”¹"):
                        # å¤„ç†ç©ºå€¼
                        new_optim_dir = None if new_optim_dir == "" else new_optim_dir
                        new_rule_category = None if new_rule_category == "" else new_rule_category
                        new_rule_class = None if new_rule_class == "" else new_rule_class

                        # è¿‡æ»¤æ‰ç©ºçš„è¾¹ç•Œç»„
                        bounds_list = [b for b in st.session_state.bounds_list if b["lower_bound"] or b["upper_bound"]]

                        if not new_q_text:
                            st.error("é—®é¢˜æ–‡æœ¬ä¸èƒ½ä¸ºç©ºï¼")
                            return

                        # è°ƒç”¨æ·»åŠ å‡½æ•°è€Œä¸æ˜¯ä¿®æ”¹å‡½æ•°
                        success, msg = add_question_to_file(
                            ttl_file_path=file_path,
                            q_text=new_q_text,
                            tag=new_tag,
                            optim_dir=new_optim_dir if new_optim_dir else "neutral",
                            overfit_risk=new_overfit_risk if new_overfit_risk else None,
                            rule_category=new_rule_category if new_rule_category else "Descriptive",
                            rule_class=new_rule_class if new_rule_class else "BaseRule",
                            rule_desc=new_rule_desc,
                            rule_recom=new_rule_recom,
                            bounds_list=[b for b in st.session_state.bounds_list if
                                         b["lower_bound"] or b["upper_bound"]]
                        )

                        if success:
                            st.success(msg)
                            st.session_state.bounds_list = [{"lower_bound": "", "upper_bound": ""}]
                        else:
                            st.error(msg)

                else:
                    # ç”¨æˆ·é€‰æ‹©äº†æ–‡ä»¶ä¸­å·²å­˜åœ¨çš„ä¸€ä¸ªé—®é¢˜
                    st.write(f"#### æ‚¨é€‰æ‹©ä¿®æ”¹/åˆ é™¤çš„é—®é¢˜ï¼š{selected_question}")
                    # è®©ç”¨æˆ·é€‰æ‹©è¦è¿›è¡Œçš„æ“ä½œ
                    operation = st.radio("è¯·é€‰æ‹©æ“ä½œ", ["ä¿®æ”¹è¯¥é—®é¢˜", "åˆ é™¤è¯¥é—®é¢˜"])

                    if operation == "ä¿®æ”¹è¯¥é—®é¢˜":
                        # æä¾›å¯ä¿®æ”¹é¡¹
                        st.write("##### åŸºæœ¬ä¿¡æ¯")
                        new_q_text = st.text_input("æ–°çš„é—®é¢˜æ–‡æœ¬(ç•™ç©ºè¡¨ç¤ºä¸æ”¹)", "")
                        new_tag = st.text_input("æ–°çš„æ ‡ç­¾(ç•™ç©ºè¡¨ç¤ºä¸æ”¹)", "")
                        new_optim_dir = st.selectbox("æ–°çš„ä¼˜åŒ–æ–¹å‘", 
                                                    ["", "maximize", "minimize", "neutral"],
                                                    format_func=lambda x: {"": "ä¿æŒä¸å˜", "maximize": "æœ€å¤§åŒ–", "minimize": "æœ€å°åŒ–", "neutral": "ä¸­æ€§"}[x])
                        new_overfit_risk = st.text_area("æ–°çš„è¿‡æ‹Ÿåˆé£é™©è¯´æ˜(ç•™ç©ºè¡¨ç¤ºä¸æ”¹)", "")

                        st.write("##### è§„åˆ™ä¿¡æ¯")
                        new_rule_category = st.selectbox("æ–°çš„è§„åˆ™ç±»åˆ«",
                                                        ["", "Constraint", "Descriptive"],
                                                        format_func=lambda x: {"": "ä¿æŒä¸å˜", "Constraint": "çº¦æŸè§„åˆ™", "Descriptive": "æè¿°æ€§è§„åˆ™"}[x])
                        new_rule_class = st.selectbox("æ–°çš„è§„åˆ™ç±»",
                                                     ["", "MSERule", "R2Rule", "MAERule", "RMSERule", "MAPERule"],
                                                     format_func=lambda x: {"": "ä¿æŒä¸å˜", "MSERule": "MSEè§„åˆ™", "R2Rule": "R2è§„åˆ™", 
                                                                          "MAERule": "MAEè§„åˆ™", "RMSERule": "RMSEè§„åˆ™", 
                                                                          "MAPERule": "MAPEè§„åˆ™"}[x])
                        new_rule_desc = st.text_area("æ–°çš„è§„åˆ™æè¿°(ç•™ç©ºè¡¨ç¤ºä¸æ”¹)", "")
                        new_rule_recom = st.text_area("æ–°çš„è§„åˆ™æ¨è(ç•™ç©ºè¡¨ç¤ºä¸æ”¹)", "")

                        # è¾¹ç•Œä¿¡æ¯éƒ¨åˆ†
                        st.write("##### è¾¹ç•Œä¿¡æ¯")
                        if "bounds_list" not in st.session_state:
                            st.session_state.bounds_list = [{"lower_bound": "", "upper_bound": ""}]

                        for i, bounds in enumerate(st.session_state.bounds_list):
                            col1, col2, col3 = st.columns([2, 2, 1])
                            with col1:
                                bounds["lower_bound"] = st.text_input(f"ä¸‹ç•Œ {i+1}", bounds["lower_bound"])
                            with col2:
                                bounds["upper_bound"] = st.text_input(f"ä¸Šç•Œ {i+1}", bounds["upper_bound"])
                            with col3:
                                if st.button("åˆ é™¤", key=f"del_{i}"):
                                    st.session_state.bounds_list.pop(i)
                                    st.rerun()

                        if st.button("æ·»åŠ æ–°çš„è¾¹ç•Œç»„"):
                            st.session_state.bounds_list.append({"lower_bound": "", "upper_bound": ""})
                            st.rerun()

                        if st.button("ç¡®è®¤ä¿®æ”¹"):
                            # å¤„ç†ç©ºå€¼
                            new_optim_dir = None if new_optim_dir == "" else new_optim_dir
                            new_rule_category = None if new_rule_category == "" else new_rule_category
                            new_rule_class = None if new_rule_class == "" else new_rule_class
                            
                            # è¿‡æ»¤æ‰ç©ºçš„è¾¹ç•Œç»„
                            bounds_list = [b for b in st.session_state.bounds_list if b["lower_bound"] or b["upper_bound"]]

                            success, msg = update_question_in_file(
                                file_path,
                                old_question=selected_question["text"],  # ä¿®å¤å
                                new_question=new_q_text or None,
                                new_tag=new_tag or None,
                                new_optim_dir=new_optim_dir,
                                new_overfit_risk=new_overfit_risk or None,
                                new_rule_category=new_rule_category,
                                new_rule_class=new_rule_class,
                                new_rule_desc=new_rule_desc or None,
                                new_rule_recom=new_rule_recom or None,
                                new_bounds_list=bounds_list if bounds_list else None
                            )
                            if success:
                                st.success(msg)
                                # æ¸…ç©ºè¾¹ç•Œåˆ—è¡¨
                                st.session_state.bounds_list = [{"lower_bound": "", "upper_bound": ""}]
                            else:
                                st.error(msg)

                    elif operation == "åˆ é™¤è¯¥é—®é¢˜":
                        if st.button("ç¡®è®¤åˆ é™¤"):
                            success, msg = delete_question_in_file(file_path, selected_question["text"])

                            if success:
                                st.success(msg)
                            else:
                                st.error(msg)

        with tab5:
            st.header("ä¿®æ”¹çŸ¥è¯†å›¾è°±åº“é…ç½®")
            st.write("### å½“å‰é…ç½®")
            
            # æ˜¾ç¤ºå½“å‰é…ç½®
            current_config = st.session_state.kb_paths
            for kb_type, path in current_config.items():
                st.text_input(
                    f"{kb_type} çŸ¥è¯†åº“è·¯å¾„",
                    value=path,
                    key=f"config_{kb_type}",
                    disabled=True
                )
            
            st.write("### ä¿®æ”¹é…ç½®")
            # åˆ›å»ºæ–°çš„é…ç½®å­—å…¸
            new_config = {}
            
            # ä¸ºæ¯ä¸ªçŸ¥è¯†åº“ç±»å‹åˆ›å»ºæ–‡ä»¶å¤¹é€‰æ‹©
            kb_types = {
                "regression": "å›å½’æ¨¡å‹çŸ¥è¯†å›¾è°±åº“",
                "classification": "åˆ†ç±»æ¨¡å‹çŸ¥è¯†å›¾è°±åº“",
                "math": "æ•°å­¦æ¨¡å‹çŸ¥è¯†å›¾è°±åº“",
                "hybrid": "æ··åˆæ¨¡å‹çŸ¥è¯†å›¾è°±åº“"
            }
            
            # ä½¿ç”¨åˆ—å¸ƒå±€å¹¶æ’æ˜¾ç¤ºé€‰æ‹©æ¡†
            cols = st.columns(2)
            for idx, (key, label) in enumerate(kb_types.items()):
                with cols[idx % 2]:
                    st.write(f"**{label}**")
                    # ä½¿ç”¨æ–‡æœ¬è¾“å…¥æ¡†æ˜¾ç¤ºå’Œç¼–è¾‘è·¯å¾„
                    new_path = st.text_input(
                        "è¾“å…¥è·¯å¾„",
                        value=current_config.get(key, ""),
                        key=f"new_{key}",
                        help="è¯·è¾“å…¥å®Œæ•´çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œä¾‹å¦‚ï¼šD:\\study\\ADSTPapp\\kg\\kgr"
                    )
                    # éªŒè¯è·¯å¾„æ˜¯å¦å­˜åœ¨
                    if new_path and not os.path.exists(new_path):
                        st.warning(f"è­¦å‘Šï¼šè·¯å¾„ '{new_path}' ä¸å­˜åœ¨ï¼")
                    
                    new_config[key] = new_path
            
            # æ·»åŠ ä¿å­˜æŒ‰é’®
            if st.button("ä¿å­˜é…ç½®"):
                # éªŒè¯æ‰€æœ‰è·¯å¾„æ˜¯å¦éƒ½å·²å¡«å†™
                if not all(new_config.values()):
                    st.error("è¯·å¡«å†™æ‰€æœ‰çŸ¥è¯†åº“è·¯å¾„ï¼")
                else:
                    # éªŒè¯æ‰€æœ‰è·¯å¾„æ˜¯å¦å­˜åœ¨
                    missing_paths = [path for path in new_config.values() if not os.path.exists(path)]
                    if missing_paths:
                        st.error(f"ä»¥ä¸‹è·¯å¾„ä¸å­˜åœ¨ï¼š{', '.join(missing_paths)}")
                    else:
                        try:
                            # æ›´æ–°é…ç½®æ–‡ä»¶
                            config_data = {"kb_paths": new_config}
                            with open('config.json', 'w', encoding='utf-8') as f:
                                json.dump(config_data, f, indent=4, ensure_ascii=False)
                            
                            # æ›´æ–°session_stateä¸­çš„è·¯å¾„
                            st.session_state.kb_paths = new_config
                            
                            st.success("é…ç½®å·²æˆåŠŸä¿å­˜ï¼")
                            st.rerun()  # åˆ·æ–°é¡µé¢ä»¥åº”ç”¨æ–°é…ç½®
                        except Exception as e:
                            st.error(f"ä¿å­˜é…ç½®æ—¶å‡ºé”™ï¼š{str(e)}")

    with right_col:
        st.markdown("### é—®é¢˜æŸ¥è¯¢")
        st.markdown("---")
        # æ”¶é›†æ‰€æœ‰çŸ¥è¯†åº“è·¯å¾„ä¸‹çš„æ–‡ä»¶
        all_ttl_paths = []
        for kb_type in ["regression", "classification", "math", "hybrid"]:
            dir_path = st.session_state.kb_paths.get(kb_type, "")
            if dir_path and os.path.exists(dir_path):
                all_ttl_paths.extend(glob.glob(os.path.join(dir_path, "*.ttl")))

        search_query = st.text_input("è¾“å…¥æŸ¥è¯¢å…³é”®è¯", "")
        if search_query:
            search_results = search_question(search_query, all_ttl_paths)

            if search_results:
                st.markdown("#### æœç´¢ç»“æœ")
                for file_path, questions in search_results.items():
                    file_name = os.path.basename(file_path)
                    with st.expander(f"ğŸ“„ {file_name}", expanded=False):
                        for q_dict in questions:
                            st.markdown(f"- **é—®é¢˜**: {q_dict['text']}")
                            if q_dict['tag']:
                                st.markdown(f"  - æ ‡ç­¾: {q_dict['tag']}")
                            if q_dict['optim_dir']:
                                st.markdown(f"  - ä¼˜åŒ–æ–¹å‘: {q_dict['optim_dir']}")
                            if q_dict['overfitRisk']:
                                st.markdown(f"  - è¿‡æ‹Ÿåˆé£é™©: {q_dict['overfitRisk']}")
                            st.markdown("---")
            else:
                st.warning(f"æœªæ‰¾åˆ°ä¸ '{search_query}' ç›¸å…³çš„é—®é¢˜ã€‚")

if __name__ == "__main__":
    main() 