import streamlit as st
import pandas as pd
import re
import os
from LocalLLMcomponents import SettingForOllama
from datasciencecomponents import DataScienceRegressionComponents,DataScienceClassifierComponents,DataEngineering,FindBestModel
from NLGcomponents import RegressionTemplateBasedTextGeneration,ClassifierTemplateBasedTextGeneration,SettingForChatGPT,AutoFindBestModel,LoadQuestionBank
from LLMcomponents import SettingForLLM
import glob

# ä»å­—ç¬¦ä¸²ä¸­æå–ç¬¬ä¸€ä¸ªæ•´æ•°ï¼Œç”¨äºè§£æåŒ¹é…ç»“æœ
def extract_first_integer(string):
    match = re.search(r"\d+", string)
    if match:
        return int(match.group())
    return 0

# åˆå§‹åŒ– LLM ä¸ç»„ä»¶
set_for_localLLM = SettingForOllama()
set_for_GPT = SettingForChatGPT()
ds_data_engineering = DataEngineering()
ds_regression = DataScienceRegressionComponents()
ds_classifier = DataScienceClassifierComponents()
nlg_reg = RegressionTemplateBasedTextGeneration()
nlg_cls = ClassifierTemplateBasedTextGeneration()
loader = LoadQuestionBank()
set_for_LLM=SettingForLLM()



def initialize_session_state():
    if 'initialized' not in st.session_state:
        st.session_state.initialized = True
        st.session_state.current_step = 1
        st.session_state.df = None
        st.session_state.dependent_var = None
        st.session_state.independent_vars = []
        st.session_state.background_text = ''
        st.session_state.llm_bg = []
        st.session_state.llm_messages = []
        st.session_state.user_questions = []
        st.session_state.recommended_models = []
        st.session_state.reg_choice = 'æ— '
        st.session_state.cls_choice = 'æ— '
        st.session_state.selected_model = None
        st.session_state.fit_results = {}
        st.session_state.single_question = ''
        st.session_state.single_answer = ''
        st.session_state.llm_mode = 'ollama'
        st.session_state.openai_model = 'gpt-3.5-turbo'
        st.session_state.openai_key = ''

        # è‡ªåŠ¨åŠ è½½é¢˜åº“æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ txt æ–‡ä»¶
        base_path = r".\apptemplates\QuestionBank\modelquestionbanks"
        txt_files = glob.glob(os.path.join(base_path, "*.txt"))
        st.session_state.model_question_bank_paths = txt_files

        # åŠ è½½é¢˜åº“å†…å®¹
        banks = {}
        for path in txt_files:
            model_name = os.path.splitext(os.path.basename(path))[0].replace('_questions', '')
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                banks[model_name] = content
                st.write(f"å·²åŠ è½½ {os.path.basename(path)}...")
            except Exception as e:
                banks[model_name] = ''
                st.warning(f"æ— æ³•åŠ è½½é¢˜åº“ {model_name}: {e}")

        st.session_state.model_question_banks = banks
        st.session_state.model_match_counts = {name: 0 for name in banks.keys()}
        st.session_state.recommended_models = []


# ç»Ÿè®¡ç”¨æˆ·é—®é¢˜ä¸å„æ¨¡å‹é¢˜åº“çš„åŒ¹é…æ¬¡æ•°
def compute_model_match_counts(user_questions):
    counts = {name: 0 for name in st.session_state.model_question_banks.keys()}
    # ç¡®ä¿èƒŒæ™¯å·²å‘é€ç»™ LLM
    # set_for_localLLM.set_chat_background([], st.session_state.background_text)  # å¦‚éœ€å¯ç”¨å¯¹è¯ä¸Šä¸‹æ–‡
    if st.session_state.llm_mode == "ollama":
        for q in user_questions:
            for model_name, full_text in st.session_state.model_question_banks.items():
                if not full_text:
                    continue
                output = set_for_localLLM.question_matching(q, full_text)
                if extract_first_integer(output) != 0:
                    counts[model_name] += 1
    else:
        for q in user_questions:
            for model_name, full_text in st.session_state.model_question_banks.items():
                if not full_text:
                    continue

                st.session_state.payload, st.session_state.llm_messages = set_for_LLM.set_payload(q, st.session_state.openai_chatmodel, st.session_state.llm_messages)
                output, st.session_state.llm_messages = set_for_LLM.send_response_receive_output(st.session_state.openai_url, st.session_state.openai_headers, st.session_state.payload, st.session_state.llm_messages)

                if extract_first_integer(output) != 0:
                    counts[model_name] += 1
    return counts

# æ­¥éª¤1ï¼šæ•°æ®ä¸Šä¼ 
def step_upload_data():
    st.header("ğŸ“¤ æ­¥éª¤1ï¼šæ•°æ®ä¸Šä¼ ")
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ CSVæ–‡ä»¶", type=["csv"], help="æ”¯æŒæ ‡å‡†CSVæ ¼å¼ï¼Œæœ€å¤§æ–‡ä»¶å¤§å°200MB"
    )
    if uploaded_file is not None:
        try:
            st.session_state.df = pd.read_csv(uploaded_file)
            st.success("âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼")
            st.session_state.data_preview_expanded = True
        except Exception as e:
            st.error(f"âŒ æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
            st.session_state.df = None
    if st.session_state.df is not None:
        with st.expander("æ•°æ®é¢„è§ˆ", expanded=st.session_state.data_preview_expanded):
            st.dataframe(st.session_state.df, height=300, use_container_width=True)
            st.subheader("æ•°æ®æ‘˜è¦")
            c1, c2, c3 = st.columns(3)
            c1.metric("æ€»è¡Œæ•°", len(st.session_state.df))
            c2.metric("å˜é‡æ•°", len(st.session_state.df.columns))
            c3.metric("ç¼ºå¤±å€¼", st.session_state.df.isna().sum().sum())
    c1, c2 = st.columns([1,1])
    with c1:
        if st.session_state.df is not None and st.button("é‡æ–°ä¸Šä¼ "):
            st.session_state.df = None
            st.rerun()
    with c2:
        if st.session_state.df is not None and st.button("ä¸‹ä¸€æ­¥ â†’", type="primary"):
            st.session_state.current_step = 2
            st.rerun()
# --- æ–°å¢éƒ¨åˆ†ï¼šè®©ç”¨æˆ·é€‰æ‹©é¢˜åº“æ–‡ä»¶å¤¹ ---
    st.divider()
    st.subheader("ğŸ“ å¯é€‰ï¼šæ›´æ¢é—®é¢˜åº“æ–‡ä»¶å¤¹")

    custom_qb_path = st.text_input("è¯·è¾“å…¥æ–°çš„é—®é¢˜åº“æ–‡ä»¶å¤¹è·¯å¾„ï¼š", value=r".\apptemplates\QuestionBank\modelquestionbanks")

    if st.button("åŠ è½½æ–°çš„é—®é¢˜åº“è·¯å¾„"):
        import glob, os
        if os.path.isdir(custom_qb_path):
            txt_files = glob.glob(os.path.join(custom_qb_path, "*.txt"))
            if not txt_files:
                st.warning("âš ï¸ å½“å‰æ–‡ä»¶å¤¹ä¸‹æœªæ‰¾åˆ°ä»»ä½• .txt é—®é¢˜åº“æ–‡ä»¶ã€‚")
            else:
                banks = {}
                for path in txt_files:
                    model_name = os.path.splitext(os.path.basename(path))[0].replace('_questions', '')
                    try:
                        with open(path, 'r', encoding='utf-8') as f:
                            banks[model_name] = f.read()
                        st.write(f"å·²åŠ è½½ï¼š{os.path.basename(path)}")
                    except Exception as e:
                        banks[model_name] = ''
                        st.warning(f"âš ï¸ åŠ è½½ {path} å¤±è´¥: {e}")
                st.session_state.model_question_banks = banks
                st.session_state.model_match_counts = {k: 0 for k in banks.keys()}
                st.success(f"âœ… æˆåŠŸåŠ è½½ {len(txt_files)} ä¸ªé¢˜åº“æ–‡ä»¶ã€‚")
        else:
            st.error("âŒ è¾“å…¥çš„è·¯å¾„ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆæ–‡ä»¶å¤¹ã€‚")




# æ­¥éª¤2ï¼šå˜é‡é€‰æ‹©
def step_select_variables():
    st.header("ğŸ“Š æ­¥éª¤2ï¼šå˜é‡é€‰æ‹©")
    if st.session_state.df is None:
        st.warning("è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶")
        st.session_state.current_step = 1
        st.rerun()
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("ç›®æ ‡å˜é‡ (Y)")
        st.session_state.dependent_var = st.selectbox("é€‰æ‹©å› å˜é‡", st.session_state.df.columns)
    with col2:
        st.subheader("ç‰¹å¾å˜é‡ (X)")
        avail = [c for c in st.session_state.df.columns if c != st.session_state.dependent_var]
        st.session_state.independent_vars = st.multiselect("é€‰æ‹©è‡ªå˜é‡", avail, default=avail[:3])
    if len(st.session_state.independent_vars) < 1:
        st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè‡ªå˜é‡")
        return
    prev, _, nxt = st.columns([1,8,1])
    with prev:
        if st.button("â† ä¸Šä¸€æ­¥"):
            st.session_state.current_step = 1
            st.rerun()
    with nxt:
        if st.button("ä¸‹ä¸€æ­¥ â†’", type="primary"):
            st.session_state.current_step = 3
            st.rerun()

# æ­¥éª¤3ï¼šç¡®è®¤å˜é‡é€‰æ‹©
def step_confirm_selection():
    st.header("âœ… æ­¥éª¤3ï¼šé€‰æ‹©ç¡®è®¤")

    st.subheader("å½“å‰é€‰æ‹©")
    st.markdown(f"""
    <div style="padding:15px; border-radius:10px; background:#f0f2f6">
    ğŸ” **åˆ†æç›®æ ‡**ï¼šé¢„æµ‹/åˆ†æ {st.session_state.dependent_var}
    ğŸ› ï¸ **ä½¿ç”¨ç‰¹å¾**ï¼š{', '.join(st.session_state.independent_vars)}
    </div>
    """, unsafe_allow_html=True)

    with st.expander("å˜é‡è¯¦ç»†ä¿¡æ¯", expanded=True):
        tab1, tab2 = st.tabs(["ç›®æ ‡å˜é‡åˆ†æ", "ç‰¹å¾å˜é‡æ¦‚è§ˆ"])

        with tab1:
            st.write(st.session_state.df[st.session_state.dependent_var].describe())
            if pd.api.types.is_numeric_dtype(st.session_state.df[st.session_state.dependent_var]):
                st.line_chart(st.session_state.df[st.session_state.dependent_var])
            else:
                st.bar_chart(st.session_state.df[st.session_state.dependent_var].value_counts())

        with tab2:
            st.dataframe(st.session_state.df[st.session_state.independent_vars].describe())

    st.divider()
    st.subheader("ğŸ¤– é€‰æ‹©å¤§è¯­è¨€æ¨¡å‹æ¥æº")

    llm_source = st.radio(
        "è¯·é€‰æ‹©ç”¨äºé—®é¢˜åŒ¹é…ä¸å›ç­”çš„LLMæ¨¡å‹æ¥æºï¼š",
        options=["ä½¿ç”¨æœ¬åœ° Ollama", "è°ƒç”¨ OpenAI API"],
        index=0,
        key="llm_source_choice"
    )

    if llm_source == "è°ƒç”¨ OpenAI API":
        st.session_state.llm_mode = "api"
        st.text_input("è¯·è¾“å…¥æ¨¡å‹åç§°", key="openai_model")
        st.text_input("è¯·è¾“å…¥ OpenAI API Key", type="password", key="openai_key")
    else:
        st.session_state.llm_mode = "ollama"

    col_prev, col_mid, col_next = st.columns([1, 8, 1])
    with col_prev:
        if st.button("â† é‡æ–°é€‰æ‹©"):
            st.session_state.current_step = 2
            st.rerun()

    with col_next:
        if st.button("å¼€å§‹å»ºæ¨¡ â†’", type="primary"):
            st.session_state.current_step = 4
            st.rerun()

# æ­¥éª¤4ï¼šè¾“å…¥æ•°æ®é›†èƒŒæ™¯çŸ¥è¯†
def step_input_background():

    st.header("ğŸŒ æ­¥éª¤4ï¼šè¾“å…¥æ•°æ®é›†èƒŒæ™¯çŸ¥è¯†")
    st.markdown("è¯·æä¾›å…³äºæ•°æ®é›†çš„èƒŒæ™¯ä¿¡æ¯ï¼Œä»¥ä¾¿ LLM åœ¨é—®é¢˜åŒ¹é…æ—¶å‚è€ƒã€‚å¯ç®€è¿°æ•°æ®æ¥æºã€å«ä¹‰ã€é¢„å¤„ç†ç­‰ã€‚æ¯”å¦‚ï¼š\n- æ•°æ®æ”¶é›†äº...\n- åŒ…å«å˜é‡...\n- æ•°æ®å·²å®Œæˆç¼ºå¤±å€¼å¡«å……ç­‰ã€‚")

    text = st.text_area("è¯·è¾“å…¥èƒŒæ™¯çŸ¥è¯†ï¼š", height=200, value=st.session_state.background_text)

    if text:
        st.session_state.background_text = text
        st.success("âœ… èƒŒæ™¯ä¿¡æ¯å·²ä¿å­˜ï¼")

        if st.session_state.llm_mode == "ollama":
            # ä½¿ç”¨æœ¬åœ° LLM
            messages, bg = set_for_localLLM.set_chat_background([], text)
            st.session_state.llm_bg = bg
            st.session_state.llm_messages = messages
        else:
            # ä½¿ç”¨ OpenAI API
            key = st.session_state.openai_key
            model = st.session_state.openai_model
            url, bg, chatmodel, headers, messages = set_for_LLM.set_chatGPT(text, key, model)

            st.session_state.llm_bg = bg
            st.session_state.llm_messages = messages
            st.session_state.openai_url = url
            st.session_state.openai_headers = headers
            st.session_state.openai_chatmodel = chatmodel

    prev, _, nxt = st.columns([1, 8, 1])
    with prev:
        if st.button("â† è¿”å›ç¡®è®¤å˜é‡"):
            st.session_state.current_step = 3
            st.rerun()
    with nxt:
        if st.button("ä¸‹ä¸€æ­¥ â†’", type="primary"):
            if not st.session_state.background_text:
                st.error("è¯·å…ˆè¾“å…¥èƒŒæ™¯çŸ¥è¯†ï¼")
            else:
                st.session_state.current_step = 5
                st.rerun()


# æ­¥éª¤5ï¼šæ‰¹é‡è¾“å…¥é—®é¢˜å¹¶æ¨èæ¨¡å‹
def step_input_questions():
    st.header("â“ æ­¥éª¤5ï¼šæ‰¹é‡è¾“å…¥é—®é¢˜")
    with st.expander("æ ¼å¼è¯´æ˜", expanded=True):
        st.markdown("1. é—®é¢˜ä¸€\n2. é—®é¢˜äºŒ\n3. é—®é¢˜ä¸‰")
    text = st.text_area("è¯·è¾“å…¥é—®é¢˜åˆ—è¡¨:", height=200)
    if text:
        parts = re.split(r"\n\s*\d+\.\s*", text.strip())
        st.session_state.user_questions = [p.strip() for p in parts if p.strip()]
    if st.session_state.user_questions:
        st.success("è§£æåˆ°ä»¥ä¸‹é—®é¢˜ï¼š")
        for i, q in enumerate(st.session_state.user_questions, 1): st.write(f"{i}. {q}")
        counts = compute_model_match_counts(st.session_state.user_questions)
        st.session_state.model_match_counts = counts
        max_cnt = max(counts.values()) if counts else 0
        recommended = [m for m,c in counts.items() if c==max_cnt and c>0]
        st.session_state.recommended_models = recommended
        st.subheader("ğŸ” åŒ¹é…ç»Ÿè®¡")
        for m,c in counts.items(): st.write(f"- {m}: {c}")
        if recommended:
            st.success(f"â­ æ¨èï¼š{', '.join(recommended)}")
        else:
            st.warning("âš ï¸ æœªåŒ¹é…åˆ°åˆé€‚æ¨¡å‹")



    prev,mid,nxt = st.columns([1,1,1])
    with prev:
        if st.button("â† è¿”å›èƒŒæ™¯"): st.session_state.current_step=4; st.rerun()
    with mid:
        if st.button("è·³è¿‡æ¨è"): st.session_state.current_step=6; st.rerun()
    with nxt:
        if st.button("ä¸‹ä¸€æ­¥ â†’"):
            if not st.session_state.recommended_models:
                st.error("è¯·å…ˆå®Œæˆæ¨¡å‹åŒ¹é…ï¼")
            else:
                st.session_state.current_step=6; st.rerun()

# æ­¥éª¤6ï¼šé€‰æ‹©å¹¶æ‹Ÿåˆæ¨¡å‹
def step_select_model():
    st.header("âš™ï¸ æ­¥éª¤6ï¼šé€‰æ‹©å¹¶æ‹Ÿåˆæ¨¡å‹")
    reg_opts = ["Statsmodels Linear Regression","Scikit-learn Linear Regression","Ridge Regression","Gradient Boosting Regression","Random Forest Regression","Lasso Regression","Bayesian Ridge Regression","Auto-fit Best Regression"]
    cls_opts = ["Logistic Regression","Linear Discriminant Analysis","SVM - Linear Kernel","Ridge Classifier","Random Forest Classifier","Decision Tree Classifier","Auto-fit Best Classifier"]
    c1,c2 = st.columns(2)
    with c1:
        st.subheader("å›å½’")
        st.session_state.reg_choice = st.radio("é€‰æ‹©ä¸€ä¸ªå›å½’æ¨¡å‹", ["æ— "]+reg_opts)
        if st.session_state.reg_choice=="Ridge Regression": st.number_input("Ridge Alpha", key='ridge_alpha')
        if st.session_state.reg_choice=="Lasso Regression": st.number_input("Lasso Alpha", key='lasso_alpha')
        if st.session_state.reg_choice=="Bayesian Ridge Regression":
            st.number_input("Alpha1", key='bayes_a1')
            st.number_input("Alpha2", key='bayes_a2')
            st.number_input("Lambda1", key='bayes_l1')
            st.number_input("Lambda2", key='bayes_l2')
        if st.session_state.reg_choice=="Auto-fit Best Regression": st.text_input("Criterion", key='reg_crit')
    with c2:
        st.subheader("åˆ†ç±»")
        st.session_state.cls_choice = st.radio("é€‰æ‹©ä¸€ä¸ªåˆ†ç±»æ¨¡å‹", ["æ— "]+cls_opts)
        if st.session_state.cls_choice=="Auto-fit Best Classifier": st.text_input("Criterion", key='cls_crit')
    if st.button("ä¸‹ä¸€æ­¥ â†’"):
        r, c = st.session_state.reg_choice, st.session_state.cls_choice
        if (r!="æ— ")==(c!="æ— "):
            st.error("åªèƒ½é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼"); return
        df = st.session_state.df
        X = df[st.session_state.independent_vars]
        y = df[st.session_state.dependent_var]
        results = {}
        if r!="æ— ":
            # æ•°æ®æ¸…æ´—
            df_clean = ds_data_engineering.clean_data(df, 0.8, Xcol=st.session_state.independent_vars, ycol=st.session_state.dependent_var)
            Xc = df_clean[st.session_state.independent_vars]
            yc = df_clean[st.session_state.dependent_var]
            # æ‹Ÿåˆå›å½’
            if r=="Statsmodels Linear Regression": coef,pval,r2 = ds_regression.train_sm_linear_regression(Xc,yc)
            elif r=="Scikit-learn Linear Regression": coef,pval,r2 = ds_regression.train_sk_linear_regression(Xc,yc)
            elif r=="Ridge Regression": coef,pval,r2 = ds_regression.train_ridge_regression(Xc,yc,st.session_state.ridge_alpha)
            elif r=="Lasso Regression": coef,pval,r2 = ds_regression.train_lasso_regression(Xc,yc,st.session_state.lasso_alpha)
            elif r=="Bayesian Ridge Regression": coef,pval,r2 = ds_regression.train_BayesianRidge_regression(
                Xc,yc,st.session_state.bayes_a1,st.session_state.bayes_a2,st.session_state.bayes_l1,st.session_state.bayes_l2)
            elif r=="Gradient Boosting Regression": coef,pval,r2,train_r2,test_r2 = ds_regression.train_gradient_boosting_regression(Xc,yc); results.update({'train_r2':train_r2,'test_r2':test_r2})
            elif r=="Random Forest Regression": coef,pval,r2,train_r2,test_r2 = ds_regression.train_random_forest_regression(Xc,yc); results.update({'train_r2':train_r2,'test_r2':test_r2})
            elif r=="Auto-fit Best Regression": name,detail,crit,_ = ds_data_engineering.find_best_regression(Xc,yc,st.session_state.dependent_var,st.session_state.reg_crit,st.session_state.independent_vars); results.update({'model_name':name,'detail':detail,'criterion':crit})
            # ç”Ÿæˆç³»æ•°ä¸På€¼è¡¨
            coef_df = ds_regression.coefficients_with_Pvalues(coef,pval,st.session_state.independent_vars)
            results.update({'coeff_df':coef_df,'r_squared':r2})
            st.session_state.selected_model = r
            st.session_state.fit_results = results
        else:
            # æ‹Ÿåˆåˆ†ç±»
            if c=="Logistic Regression": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_logistic_regression(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="Linear Discriminant Analysis": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_linear_discriminant_analysis(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="SVM - Linear Kernel": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_SVC_classifier(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="Ridge Classifier": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_ridge_classifier(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="Random Forest Classifier": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_random_forest_classifier(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="Decision Tree Classifier": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_decision_tree_classifier(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            else: name,detail,crit,_ = ds_data_engineering.find_best_classifier(X,y,st.session_state.dependent_var,st.session_state.cls_crit,st.session_state.independent_vars); results.update({'model_name':name,'detail':detail,'criterion':crit}); model=None; coeff_df=None; acc=None; train_acc=None; test_acc=None
            results.update({'coeff_df':coeff_df,'accuracy':acc,'train_accuracy':train_acc,'test_accuracy':test_acc})
            st.session_state.selected_model = c
            st.session_state.fit_results = results
        st.session_state.current_step = 7
        st.rerun()

# æ­¥éª¤7ï¼šå•é¢˜æé—®
def step_input_single_question():
    st.header("â“ æ­¥éª¤7ï¼šå•é¢˜æé—®")
    st.session_state.single_question = st.text_input("è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", value="", key='q_input')
    if st.session_state.single_question and st.button("æäº¤é—®é¢˜ â†’"):
        q = st.session_state.single_question
        # åŒ¹é…æ¨¡æ¿èŠ‚å·
        if 'Regression' in st.session_state.selected_model:
            content = loader.load_regression_questions()
        else:
            content = loader.load_classifier_questions()
        if st.session_state.llm_mode == "ollama":
            sec = extract_first_integer(set_for_localLLM.question_matching(q, content))
        else:
            st.session_state.payload, st.session_state.llm_messages = set_for_LLM.set_payload(q,
                                                                                              st.session_state.openai_chatmodel,
                                                                                              st.session_state.llm_messages)
            output, st.session_state.llm_messages = set_for_LLM.send_response_receive_output(
                st.session_state.openai_url, st.session_state.openai_headers, st.session_state.payload,
                st.session_state.llm_messages)

            sec = extract_first_integer(output)
        # è·å–æ¨¡æ¿é—®ç­”
        if 'Regression' in st.session_state.selected_model:
            coef_df = st.session_state.fit_results.get('coeff_df')
            if sec==1:
                qs,ans = nlg_reg.Q_and_A_about_R2(coef_df, st.session_state.dependent_var, st.session_state.selected_model, st.session_state.fit_results.get('r_squared'))
            elif sec==2:
                qs,ans = nlg_reg.Q_and_A_about_coefficients(coef_df, st.session_state.dependent_var)
            elif sec==3:
                qs,ans = nlg_reg.Q_and_A_about_importance(st.session_state.independent_vars, st.session_state.dependent_var, st.session_state.fit_results.get('coeff'), coef_df)
            elif sec==4:
                qs,ans = nlg_reg.Q_and_A_about_pvalues(coef_df, st.session_state.dependent_var)
            elif sec==5:
                qs,ans = nlg_reg.Q_and_A_about_ML_importance(st.session_state.independent_vars, st.session_state.dependent_var, coef_df)
            elif sec==6:
                qs,ans = nlg_reg.Q_and_A_about_ML_overfit(st.session_state.fit_results.get('train_r2'), st.session_state.dependent_var, st.session_state.fit_results.get('test_r2'))
            else:
                qs,ans = [],[]
        else:
            coeff_df = st.session_state.fit_results.get('coeff_df')
            if sec==1:
                qs,ans = nlg_cls.Q_and_A_about_accuracy(st.session_state.fit_results.get('accuracy'), st.session_state.selected_model)
            elif sec==2:
                qs,ans = nlg_cls.Q_and_A_about_coefficients(coeff_df, st.session_state.dependent_var)
            elif sec==3:
                qs,ans = nlg_cls.Q_and_A_about_importance(st.session_state.fit_results.get('coeff'), st.session_state.dependent_var, coeff_df, st.session_state.fit_results.get('model').classes_)
            elif sec==4:
                qs,ans = nlg_cls.Q_and_A_about_ML_overfit(st.session_state.fit_results.get('train_accuracy'), st.session_state.fit_results.get('test_accuracy'))
            elif sec==5:
                qs,ans = nlg_cls.Q_and_A_about_ML_importance(coeff_df, st.session_state.dependent_var)
            else:
                qs,ans = [],[]
        # åŸºäºæ¨¡æ¿å›ç­”æ›´æ–°
        # default_answer = set_for_GPT.answer_update(q, qs, ans)

        default_answer="The user's question is:"+q+"\nThe corresponding predefined template questions are: "+qs+"\nSo now, please answer the user's question based on the analysis results and the above content."

        if st.session_state.llm_mode == "ollama":
            out, msgs = set_for_localLLM.send_response_receive_output(default_answer, st.session_state.llm_bg, st.session_state.llm_messages)
            st.session_state.llm_messages = msgs
            st.session_state.single_answer = out
        else:
            st.session_state.payload, st.session_state.llm_messages = set_for_LLM.set_payload(q,
                                                                                              st.session_state.openai_chatmodel,
                                                                                              st.session_state.llm_messages)
            out, msgs = set_for_LLM.send_response_receive_output(
                st.session_state.openai_url, st.session_state.openai_headers, st.session_state.payload,
                st.session_state.llm_messages)

            st.session_state.llm_messages = msgs
            st.session_state.single_answer = out
        st.session_state.current_step = 8
        st.rerun()

# æ­¥éª¤8ï¼šæ˜¾ç¤ºå›ç­”å¹¶å¯å†æ¬¡æé—®
def step_show_single_answer():
     st.header("ğŸ’¬ æ­¥éª¤8ï¼šå›ç­”ç»“æœ")
     # æ˜¾ç¤ºå‰ä¸€æ­¥çš„å›ç­”
     st.write(st.session_state.single_answer)
     # æŒ‰é’®æ ï¼šå†æ¬¡æé—® æˆ– ç”Ÿæˆæ€»ç»“
     col_reask, col_summary = st.columns([1,1])
     with col_reask:
         if st.button("å†æ¬¡æé—®"):
             st.session_state.current_step = 7
             st.rerun()
     with col_summary:
         if st.button("ç”Ÿæˆæ€»ç»“"):
             # æ„é€ æ€»ç»“æç¤ºè¯­
             summary_prompt = (
                 "Please summarize all the above Q&A interactions into a concise report focusing on the user's questions "
                 "about the dataset and the chosen model, without explaining the process."
             )
             # è°ƒç”¨ LLM ç”Ÿæˆæ€»ç»“
             if st.session_state.llm_mode == "ollama":
                 out, msgs = set_for_localLLM.send_response_receive_output(
                     summary_prompt,
                     st.session_state.llm_bg,
                     st.session_state.llm_messages
                 )
                 # æ›´æ–°æ¶ˆæ¯å†å²å¹¶ä¿å­˜ summary
                 st.session_state.llm_messages = msgs
                 st.session_state.summary = out
             else:
                 st.session_state.payload, st.session_state.llm_messages = set_for_LLM.set_payload(summary_prompt,
                                                                                                   st.session_state.openai_chatmodel,
                                                                                                   st.session_state.llm_messages)
                 out, msgs = set_for_LLM.send_response_receive_output(
                     st.session_state.openai_url, st.session_state.openai_headers, st.session_state.payload,
                     st.session_state.llm_messages)
                 # æ›´æ–°æ¶ˆæ¯å†å²å¹¶ä¿å­˜ summary
                 st.session_state.llm_messages = msgs
                 st.session_state.summary = out
             # åœ¨ç•Œé¢æ˜¾ç¤ºæ€»ç»“
             st.subheader("ğŸ“„ æ€»ç»“æŠ¥å‘Š")
             st.write(st.session_state.summary)
             # å°†èŠå¤©è®°å½•ä¿å­˜ä¸º docxï¼Œé€šçŸ¥ç”¨æˆ·
             set_for_localLLM.save_chat_history_to_docx(st.session_state.llm_messages)
             st.success("âœ… æŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜åœ¨æœåŠ¡å™¨æ ¹ç›®å½•ä¸‹çš„ data_report.docx")

 # ä¸»æµç¨‹
def main():
     st.set_page_config(page_title="æ™ºèƒ½æ•°æ®åˆ†æåŠ©æ‰‹", layout="wide")
     initialize_session_state()
     steps=["ä¸Šä¼ ","å˜é‡","ç¡®è®¤","èƒŒæ™¯","æ‰¹é‡é—®","æ¨¡å‹é€‰æ‹©","å•é¢˜é—®ç­”","ç­”æ¡ˆå±•ç¤º"]
     st.progress((st.session_state.current_step-1)/len(steps))
     if st.session_state.current_step==1: step_upload_data()
     elif st.session_state.current_step==2: step_select_variables()
     elif st.session_state.current_step==3: step_confirm_selection()
     elif st.session_state.current_step==4: step_input_background()
     elif st.session_state.current_step==5: step_input_questions()
     elif st.session_state.current_step==6: step_select_model()
     elif st.session_state.current_step==7: step_input_single_question()
     elif st.session_state.current_step==8: step_show_single_answer()

if __name__=="__main__":
     main()

