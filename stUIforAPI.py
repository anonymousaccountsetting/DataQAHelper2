import streamlit as st
import pandas as pd
import re
import os
from LocalLLMcomponents import SettingForOllama
from datasciencecomponents import DataScienceRegressionComponents,DataScienceClassifierComponents,DataEngineering,FindBestModel
from NLGcomponents import RegressionTemplateBasedTextGeneration,ClassifierTemplateBasedTextGeneration,SettingForChatGPT,AutoFindBestModel,LoadQuestionBank
from LLMcomponents import SettingForLLM
import glob

# ä»å­—ç¬¦ä¸²ä¸­æå–ç¬¬ä¸€ä¸ªæ•´æ•°ï¼Œç”¨äºè§£æåŒ¹é…ç»“æœ
def extract_first_integer(string):
    match = re.search(r"\d+", string)
    if match:
        return int(match.group())
    return 0

# åˆå§‹åŒ– LLM ä¸ç»„ä»¶
set_for_localLLM = SettingForOllama()
set_for_GPT = SettingForChatGPT()
ds_data_engineering = DataEngineering()
ds_regression = DataScienceRegressionComponents()
ds_classifier = DataScienceClassifierComponents()
nlg_reg = RegressionTemplateBasedTextGeneration()
nlg_cls = ClassifierTemplateBasedTextGeneration()
loader = LoadQuestionBank()
set_for_LLM=SettingForLLM()



def initialize_session_state():
    if 'initialized' not in st.session_state:
        st.session_state.initialized = True
        st.session_state.current_step = 1
        st.session_state.df = None
        st.session_state.dependent_var = None
        st.session_state.independent_vars = []
        st.session_state.background_text = ''
        st.session_state.llm_bg = []
        st.session_state.llm_messages = []
        st.session_state.user_questions = []
        st.session_state.recommended_models = []
        st.session_state.reg_choice = 'None'
        st.session_state.cls_choice = 'None'
        st.session_state.selected_model = None
        st.session_state.fit_results = {}
        st.session_state.single_question = ''
        st.session_state.single_answer = ''
        st.session_state.llm_mode = 'ollama'
        st.session_state.openai_model = 'gpt-3.5-turbo'
        st.session_state.openai_key = ''

        # è‡ªåŠ¨åŠ è½½é¢˜åº“æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ txt æ–‡ä»¶
        base_path = r".\apptemplates\QuestionBank\modelquestionbanks"
        txt_files = glob.glob(os.path.join(base_path, "*.txt"))
        st.session_state.model_question_bank_paths = txt_files

        # åŠ è½½é¢˜åº“å†…å®¹
        banks = {}
        for path in txt_files:
            model_name = os.path.splitext(os.path.basename(path))[0].replace('_questions', '')
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                banks[model_name] = content
                st.write(f"Loaded {os.path.basename(path)}...")
            except Exception as e:
                banks[model_name] = ''
                st.warning(f"Unable to load question bank {model_name}: {e}")

        st.session_state.model_question_banks = banks
        st.session_state.model_match_counts = {name: 0 for name in banks.keys()}
        st.session_state.recommended_models = []


# ç»Ÿè®¡ç”¨æˆ·é—®é¢˜ä¸å„æ¨¡å‹é¢˜åº“çš„åŒ¹é…æ¬¡æ•°
def compute_model_match_counts(user_questions):
    counts = {name: 0 for name in st.session_state.model_question_banks.keys()}
    # ç¡®ä¿èƒŒæ™¯å·²å‘é€ç»™ LLM
    # set_for_localLLM.set_chat_background([], st.session_state.background_text)  # å¦‚éœ€å¯ç”¨å¯¹è¯ä¸Šä¸‹æ–‡
    if st.session_state.llm_mode == "ollama":
        for q in user_questions:
            for model_name, full_text in st.session_state.model_question_banks.items():
                if not full_text:
                    continue
                output = set_for_localLLM.question_matching(q, full_text)
                if extract_first_integer(output) != 0:
                    counts[model_name] += 1
    else:
        for q in user_questions:
            for model_name, full_text in st.session_state.model_question_banks.items():
                if not full_text:
                    continue

                st.session_state.payload, st.session_state.llm_messages = set_for_LLM.set_payload(q, st.session_state.openai_chatmodel, st.session_state.llm_messages)
                output, st.session_state.llm_messages = set_for_LLM.send_response_receive_output(st.session_state.openai_url, st.session_state.openai_headers, st.session_state.payload, st.session_state.llm_messages)

                if extract_first_integer(output) != 0:
                    counts[model_name] += 1
    return counts

# æ­¥éª¤1ï¼šæ•°æ®ä¸Šä¼ 
def step_upload_data():
    st.header("ğŸ“¤ Step 1: Upload Data")
    uploaded_file = st.file_uploader(
        "Upload CSV file", type=["csv"], help="Standard CSV supported, max size 200MB"
    )
    if uploaded_file is not None:
        try:
            st.session_state.df = pd.read_csv(uploaded_file)
            st.success("âœ… File uploaded successfully!")
            st.session_state.data_preview_expanded = True
        except Exception as e:
            st.error(f"âŒ File read error: {e}")
            st.session_state.df = None
    if st.session_state.df is not None:
        with st.expander("Data preview", expanded=st.session_state.data_preview_expanded):
            st.dataframe(st.session_state.df, height=300, use_container_width=True)
            st.subheader("Data summary")
            c1, c2, c3 = st.columns(3)
            c1.metric("Total rows", len(st.session_state.df))
            c2.metric("Number of variables", len(st.session_state.df.columns))
            c3.metric("Missing values", st.session_state.df.isna().sum().sum())
    c1, c2 = st.columns([1,1])
    with c1:
        if st.session_state.df is not None and st.button("Re-upload"):
            st.session_state.df = None
            st.rerun()
    with c2:
        if st.session_state.df is not None and st.button("Next  â†’", type="primary"):
            st.session_state.current_step = 2
            st.rerun()
# --- æ–°å¢éƒ¨åˆ†ï¼šè®©ç”¨æˆ·é€‰æ‹©é¢˜åº“æ–‡ä»¶å¤¹ ---
    st.divider()
    st.subheader("ğŸ“ Optional: Change question bank folder")

    custom_qb_path = st.text_input("Enter a new question bank folder path:", value=r".\apptemplates\QuestionBank\modelquestionbanks")

    if st.button("Load new question bank path"):
        import glob, os
        if os.path.isdir(custom_qb_path):
            txt_files = glob.glob(os.path.join(custom_qb_path, "*.txt"))
            if not txt_files:
                st.warning("âš ï¸ No .txt question bank files found in this folder.")
            else:
                banks = {}
                for path in txt_files:
                    model_name = os.path.splitext(os.path.basename(path))[0].replace('_questions', '')
                    try:
                        with open(path, 'r', encoding='utf-8') as f:
                            banks[model_name] = f.read()
                        st.write(f"Loadedï¼š{os.path.basename(path)}")
                    except Exception as e:
                        banks[model_name] = ''
                        st.warning(f"âš ï¸ Failed to load {path}: {e}")
                st.session_state.model_question_banks = banks
                st.session_state.model_match_counts = {k: 0 for k in banks.keys()}
                st.success(f"âœ… Successfully loaded {len(txt_files)} question bank files.")
        else:
            st.error("âŒ The provided path is not a valid folder.")




# æ­¥éª¤2ï¼šå˜é‡é€‰æ‹©
def step_select_variables():
    st.header("ğŸ“Š Step 2: Variable Selection")
    if st.session_state.df is None:
        st.warning("Please upload the data file first")
        st.session_state.current_step = 1
        st.rerun()
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Target Variable (Y)")
        st.session_state.dependent_var = st.selectbox("Select a dependent variable", st.session_state.df.columns)
    with col2:
        st.subheader("Feature Variable(s) (X)")
        avail = [c for c in st.session_state.df.columns if c != st.session_state.dependent_var]
        st.session_state.independent_vars = st.multiselect("Select the independent variable(s)", avail, default=avail[:3])
    if len(st.session_state.independent_vars) < 1:
        st.error("Please select at least one independent variable")
        return
    prev, _, nxt = st.columns([1,8,1])
    with prev:
        if st.button("â† Back"):
            st.session_state.current_step = 1
            st.rerun()
    with nxt:
        if st.button("Next  â†’", type="primary"):
            st.session_state.current_step = 3
            st.rerun()

# æ­¥éª¤3ï¼šç¡®è®¤å˜é‡é€‰æ‹©
def step_confirm_selection():
    st.header("âœ… Step 3: Confirm Selection")

    st.subheader("Current Selection")
    st.markdown(f"""
    <div style="padding:15px; border-radius:10px; background:#f0f2f6">
    ğŸ” **Analysis Target**ï¼šPredict/Analyze {st.session_state.dependent_var}
    ğŸ› ï¸ **Features Used**ï¼š{', '.join(st.session_state.independent_vars)}
    </div>
    """, unsafe_allow_html=True)

    with st.expander("Variable Details", expanded=True):
        tab1, tab2 = st.tabs(["Target Variable Analysis", "Feature Overview"])

        with tab1:
            st.write(st.session_state.df[st.session_state.dependent_var].describe())
            if pd.api.types.is_numeric_dtype(st.session_state.df[st.session_state.dependent_var]):
                st.line_chart(st.session_state.df[st.session_state.dependent_var])
            else:
                st.bar_chart(st.session_state.df[st.session_state.dependent_var].value_counts())

        with tab2:
            st.dataframe(st.session_state.df[st.session_state.independent_vars].describe())

    st.divider()
    st.subheader("ğŸ¤– Choose LLM Source")

    llm_source = st.radio(
        "Select the LLM source for question matching and answering:",
        options=["Use local Ollama", "Call OpenAI API"],
        index=0,
        key="llm_source_choice"
    )

    if llm_source == "Call OpenAI API":
        st.session_state.llm_mode = "api"
        st.text_input("Enter model name", key="openai_model")
        st.text_input("Enter OpenAI API Key", type="password", key="openai_key")
    else:
        st.session_state.llm_mode = "ollama"

    col_prev, col_mid, col_next = st.columns([1, 8, 1])
    with col_prev:
        if st.button("â† Reselect"):
            st.session_state.current_step = 2
            st.rerun()

    with col_next:
        if st.button("Start Modeling â†’", type="primary"):
            st.session_state.current_step = 4
            st.rerun()

# æ­¥éª¤4ï¼šè¾“å…¥æ•°æ®é›†èƒŒæ™¯çŸ¥è¯†
def step_input_background():

    st.header("ğŸŒ Step 4: Enter Dataset Background")
    st.markdown("Provide background information about the dataset for the LLM to reference during question matching. ")

    text = st.text_area("Enter background knowledgeï¼š", height=200, value=st.session_state.background_text)

    if text:
        st.session_state.background_text = text
        st.success("âœ… Background information saved!")

        if st.session_state.llm_mode == "ollama":
            # ä½¿ç”¨æœ¬åœ° LLM
            messages, bg = set_for_localLLM.set_chat_background([], text)
            st.session_state.llm_bg = bg
            st.session_state.llm_messages = messages
        else:
            # ä½¿ç”¨ OpenAI API
            key = st.session_state.openai_key
            model = st.session_state.openai_model
            url, bg, chatmodel, headers, messages = set_for_LLM.set_chatGPT(text, key, model)

            st.session_state.llm_bg = bg
            st.session_state.llm_messages = messages
            st.session_state.openai_url = url
            st.session_state.openai_headers = headers
            st.session_state.openai_chatmodel = chatmodel

    prev, _, nxt = st.columns([1, 8, 1])
    with prev:
        if st.button("â† Back to confirmation"):
            st.session_state.current_step = 3
            st.rerun()
    with nxt:
        if st.button("Next  â†’", type="primary"):
            if not st.session_state.background_text:
                st.error("Please enter background knowledge first!")
            else:
                st.session_state.current_step = 5
                st.rerun()


# æ­¥éª¤5ï¼šæ‰¹é‡è¾“å…¥é—®é¢˜å¹¶æ¨èæ¨¡å‹
def step_input_questions():
    st.header("â“ Step 5: Batch Input Questions")
    with st.expander("Format instructions", expanded=True):
        st.markdown("1. Question one\n2. Question two\n3. Question three")
    text = st.text_area("Enter question list:", height=200)
    if text:
        parts = re.split(r"\n\s*\d+\.\s*", text.strip())
        st.session_state.user_questions = [p.strip() for p in parts if p.strip()]
    if st.session_state.user_questions:
        st.success("Parsed the following questionsï¼š")
        for i, q in enumerate(st.session_state.user_questions, 1): st.write(f"{i}. {q}")
        counts = compute_model_match_counts(st.session_state.user_questions)
        st.session_state.model_match_counts = counts
        max_cnt = max(counts.values()) if counts else 0
        recommended = [m for m,c in counts.items() if c==max_cnt and c>0]
        st.session_state.recommended_models = recommended
        st.subheader("ğŸ” Matching Statistics")
        for m,c in counts.items(): st.write(f"- {m}: {c}")
        if recommended:
            st.success(f"â­ Recommendedï¼š{', '.join(recommended)}")
        else:
            st.warning("âš ï¸ No suitable model matched")



    prev,mid,nxt = st.columns([1,1,1])
    with prev:
        if st.button("â† Back to background"): st.session_state.current_step=4; st.rerun()
    with mid:
        if st.button("Skip recommendation"): st.session_state.current_step=6; st.rerun()
    with nxt:
        if st.button("Next  â†’"):
            if not st.session_state.recommended_models:
                st.error("Please complete model matching first!")
            else:
                st.session_state.current_step=6; st.rerun()

# æ­¥éª¤6ï¼šé€‰æ‹©å¹¶æ‹Ÿåˆæ¨¡å‹
def step_select_model():
    st.header("âš™ï¸ Step 6: Select and Fit Model")
    reg_opts = ["Statsmodels Linear Regression","Scikit-learn Linear Regression","Ridge Regression","Gradient Boosting Regression","Random Forest Regression","Lasso Regression","Bayesian Ridge Regression","Auto-fit Best Regression"]
    cls_opts = ["Logistic Regression","Linear Discriminant Analysis","SVM - Linear Kernel","Ridge Classifier","Random Forest Classifier","Decision Tree Classifier","Auto-fit Best Classifier"]
    c1,c2 = st.columns(2)
    with c1:
        st.subheader("Regression")
        st.session_state.reg_choice = st.radio("Select a regression model", ["None"]+reg_opts)
        if st.session_state.reg_choice=="Ridge Regression": st.number_input("Ridge Alpha", key='ridge_alpha')
        if st.session_state.reg_choice=="Lasso Regression": st.number_input("Lasso Alpha", key='lasso_alpha')
        if st.session_state.reg_choice=="Bayesian Ridge Regression":
            st.number_input("Alpha1", key='bayes_a1')
            st.number_input("Alpha2", key='bayes_a2')
            st.number_input("Lambda1", key='bayes_l1')
            st.number_input("Lambda2", key='bayes_l2')
        if st.session_state.reg_choice=="Auto-fit Best Regression": st.text_input("Criterion", key='reg_crit')
    with c2:
        st.subheader("Classification")
        st.session_state.cls_choice = st.radio("Select a classification model", ["None"]+cls_opts)
        if st.session_state.cls_choice=="Auto-fit Best Classifier": st.text_input("Criterion", key='cls_crit')
    if st.button("Next  â†’"):
        r, c = st.session_state.reg_choice, st.session_state.cls_choice
        if (r!="None")==(c!="None"):
            st.error("Only one model can be selected!"); return
        df = st.session_state.df
        X = df[st.session_state.independent_vars]
        y = df[st.session_state.dependent_var]
        results = {}
        if r!="None":
            # æ•°æ®æ¸…æ´—
            df_clean = ds_data_engineering.clean_data(df, 0.8, Xcol=st.session_state.independent_vars, ycol=st.session_state.dependent_var)
            Xc = df_clean[st.session_state.independent_vars]
            yc = df_clean[st.session_state.dependent_var]
            # æ‹Ÿåˆå›å½’
            if r=="Statsmodels Linear Regression": coef,pval,r2 = ds_regression.train_sm_linear_regression(Xc,yc)
            elif r=="Scikit-learn Linear Regression": coef,pval,r2 = ds_regression.train_sk_linear_regression(Xc,yc)
            elif r=="Ridge Regression": coef,pval,r2 = ds_regression.train_ridge_regression(Xc,yc,st.session_state.ridge_alpha)
            elif r=="Lasso Regression": coef,pval,r2 = ds_regression.train_lasso_regression(Xc,yc,st.session_state.lasso_alpha)
            elif r=="Bayesian Ridge Regression": coef,pval,r2 = ds_regression.train_BayesianRidge_regression(
                Xc,yc,st.session_state.bayes_a1,st.session_state.bayes_a2,st.session_state.bayes_l1,st.session_state.bayes_l2)
            elif r=="Gradient Boosting Regression": coef,pval,r2,train_r2,test_r2 = ds_regression.train_gradient_boosting_regression(Xc,yc); results.update({'train_r2':train_r2,'test_r2':test_r2})
            elif r=="Random Forest Regression": coef,pval,r2,train_r2,test_r2 = ds_regression.train_random_forest_regression(Xc,yc); results.update({'train_r2':train_r2,'test_r2':test_r2})
            elif r=="Auto-fit Best Regression": name,detail,crit,_ = ds_data_engineering.find_best_regression(Xc,yc,st.session_state.dependent_var,st.session_state.reg_crit,st.session_state.independent_vars); results.update({'model_name':name,'detail':detail,'criterion':crit})
            # ç”Ÿæˆç³»æ•°ä¸På€¼è¡¨
            coef_df = ds_regression.coefficients_with_Pvalues(coef,pval,st.session_state.independent_vars)
            results.update({'coeff_df':coef_df,'r_squared':r2})
            st.session_state.selected_model = r
            st.session_state.fit_results = results
        else:
            # æ‹Ÿåˆåˆ†ç±»
            if c=="Logistic Regression": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_logistic_regression(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="Linear Discriminant Analysis": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_linear_discriminant_analysis(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="SVM - Linear Kernel": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_SVC_classifier(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="Ridge Classifier": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_ridge_classifier(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="Random Forest Classifier": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_random_forest_classifier(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            elif c=="Decision Tree Classifier": model,coeff_df,acc,coeff,train_acc,test_acc = ds_classifier.train_decision_tree_classifier(X,y,st.session_state.independent_vars,st.session_state.dependent_var)
            else: name,detail,crit,_ = ds_data_engineering.find_best_classifier(X,y,st.session_state.dependent_var,st.session_state.cls_crit,st.session_state.independent_vars); results.update({'model_name':name,'detail':detail,'criterion':crit}); model=None; coeff_df=None; acc=None; train_acc=None; test_acc=None
            results.update({'coeff_df':coeff_df,'accuracy':acc,'train_accuracy':train_acc,'test_accuracy':test_acc})
            st.session_state.selected_model = c
            st.session_state.fit_results = results
        st.session_state.current_step = 7
        st.rerun()

# æ­¥éª¤7ï¼šå•é¢˜æé—®
def step_input_single_question():
    st.header("â“ Step 7: Ask a Single Question")
    st.session_state.single_question = st.text_input("Enter your questionï¼š", value="", key='q_input')
    if st.session_state.single_question and st.button("Submit question â†’"):
        q = st.session_state.single_question
        # åŒ¹é…æ¨¡æ¿èŠ‚å·
        if 'Regression' in st.session_state.selected_model:
            content = loader.load_regression_questions()
        else:
            content = loader.load_classifier_questions()
        if st.session_state.llm_mode == "ollama":
            sec = extract_first_integer(set_for_localLLM.question_matching(q, content))
        else:
            st.session_state.payload, st.session_state.llm_messages = set_for_LLM.set_payload(q,
                                                                                              st.session_state.openai_chatmodel,
                                                                                              st.session_state.llm_messages)
            output, st.session_state.llm_messages = set_for_LLM.send_response_receive_output(
                st.session_state.openai_url, st.session_state.openai_headers, st.session_state.payload,
                st.session_state.llm_messages)

            sec = extract_first_integer(output)
        # è·å–æ¨¡æ¿é—®ç­”
        if 'Regression' in st.session_state.selected_model:
            coef_df = st.session_state.fit_results.get('coeff_df')
            if sec==1:
                qs,ans = nlg_reg.Q_and_A_about_R2(coef_df, st.session_state.dependent_var, st.session_state.selected_model, st.session_state.fit_results.get('r_squared'))
            elif sec==2:
                qs,ans = nlg_reg.Q_and_A_about_coefficients(coef_df, st.session_state.dependent_var)
            elif sec==3:
                qs,ans = nlg_reg.Q_and_A_about_importance(st.session_state.independent_vars, st.session_state.dependent_var, st.session_state.fit_results.get('coeff'), coef_df)
            elif sec==4:
                qs,ans = nlg_reg.Q_and_A_about_pvalues(coef_df, st.session_state.dependent_var)
            elif sec==5:
                qs,ans = nlg_reg.Q_and_A_about_ML_importance(st.session_state.independent_vars, st.session_state.dependent_var, coef_df)
            elif sec==6:
                qs,ans = nlg_reg.Q_and_A_about_ML_overfit(st.session_state.fit_results.get('train_r2'), st.session_state.dependent_var, st.session_state.fit_results.get('test_r2'))
            else:
                qs,ans = [],[]
        else:
            coeff_df = st.session_state.fit_results.get('coeff_df')
            if sec==1:
                qs,ans = nlg_cls.Q_and_A_about_accuracy(st.session_state.fit_results.get('accuracy'), st.session_state.selected_model)
            elif sec==2:
                qs,ans = nlg_cls.Q_and_A_about_coefficients(coeff_df, st.session_state.dependent_var)
            elif sec==3:
                qs,ans = nlg_cls.Q_and_A_about_importance(st.session_state.fit_results.get('coeff'), st.session_state.dependent_var, coeff_df, st.session_state.fit_results.get('model').classes_)
            elif sec==4:
                qs,ans = nlg_cls.Q_and_A_about_ML_overfit(st.session_state.fit_results.get('train_accuracy'), st.session_state.fit_results.get('test_accuracy'))
            elif sec==5:
                qs,ans = nlg_cls.Q_and_A_about_ML_importance(coeff_df, st.session_state.dependent_var)
            else:
                qs,ans = [],[]
        # åŸºäºæ¨¡æ¿å›ç­”æ›´æ–°
        default_answer = set_for_GPT.answer_update(q, qs, ans)
        if st.session_state.llm_mode == "ollama":
            out, msgs = set_for_localLLM.send_response_receive_output(default_answer, st.session_state.llm_bg, st.session_state.llm_messages)
            st.session_state.llm_messages = msgs
            st.session_state.single_answer = out
        else:
            st.session_state.payload, st.session_state.llm_messages = set_for_LLM.set_payload(q,
                                                                                              st.session_state.openai_chatmodel,
                                                                                              st.session_state.llm_messages)
            out, msgs = set_for_LLM.send_response_receive_output(
                st.session_state.openai_url, st.session_state.openai_headers, st.session_state.payload,
                st.session_state.llm_messages)

            st.session_state.llm_messages = msgs
            st.session_state.single_answer = out
        st.session_state.current_step = 8
        st.rerun()

# æ­¥éª¤8ï¼šæ˜¾ç¤ºå›ç­”å¹¶å¯å†æ¬¡æé—®
def step_show_single_answer():
     st.header("ğŸ’¬ Step 8: Answer Result")
     # æ˜¾ç¤ºå‰ä¸€æ­¥çš„å›ç­”
     st.write(st.session_state.single_answer)
     # æŒ‰é’®æ ï¼šå†æ¬¡æé—® æˆ– ç”Ÿæˆæ€»ç»“
     col_reask, col_summary = st.columns([1,1])
     with col_reask:
         if st.button("Ask again"):
             st.session_state.current_step = 7
             st.rerun()
     with col_summary:
         if st.button("Generate summary"):
             # æ„é€ æ€»ç»“æç¤ºè¯­
             summary_prompt = (
                 "Please summarize all the above Q&A interactions into a concise report focusing on the user's questions "
                 "about the dataset and the chosen model, without explaining the process."
             )
             # è°ƒç”¨ LLM ç”Ÿæˆæ€»ç»“
             if st.session_state.llm_mode == "ollama":
                 out, msgs = set_for_localLLM.send_response_receive_output(
                     summary_prompt,
                     st.session_state.llm_bg,
                     st.session_state.llm_messages
                 )
                 # æ›´æ–°æ¶ˆæ¯å†å²å¹¶ä¿å­˜ summary
                 st.session_state.llm_messages = msgs
                 st.session_state.summary = out
             else:
                 st.session_state.payload, st.session_state.llm_messages = set_for_LLM.set_payload(summary_prompt,
                                                                                                   st.session_state.openai_chatmodel,
                                                                                                   st.session_state.llm_messages)
                 out, msgs = set_for_LLM.send_response_receive_output(
                     st.session_state.openai_url, st.session_state.openai_headers, st.session_state.payload,
                     st.session_state.llm_messages)
                 # æ›´æ–°æ¶ˆæ¯å†å²å¹¶ä¿å­˜ summary
                 st.session_state.llm_messages = msgs
                 st.session_state.summary = out
             # åœ¨ç•Œé¢æ˜¾ç¤ºæ€»ç»“
             st.subheader("ğŸ“„ Summary Report")
             st.write(st.session_state.summary)
             # å°†èŠå¤©è®°å½•ä¿å­˜ä¸º docxï¼Œé€šçŸ¥ç”¨æˆ·
             set_for_localLLM.save_chat_history_to_docx(st.session_state.llm_messages)
             st.success("âœ… Report generated and saved as data_report.docx in the server root directory.")

 # ä¸»æµç¨‹
def main():
     st.set_page_config(page_title="Intelligent Data Analysis Assistant", layout="wide")
     initialize_session_state()
     steps=["ä¸Šä¼ ","å˜é‡","ç¡®è®¤","èƒŒæ™¯","æ‰¹é‡é—®","æ¨¡å‹é€‰æ‹©","å•é¢˜é—®ç­”","ç­”æ¡ˆå±•ç¤º"]
     st.progress((st.session_state.current_step-1)/len(steps))
     if st.session_state.current_step==1: step_upload_data()
     elif st.session_state.current_step==2: step_select_variables()
     elif st.session_state.current_step==3: step_confirm_selection()
     elif st.session_state.current_step==4: step_input_background()
     elif st.session_state.current_step==5: step_input_questions()
     elif st.session_state.current_step==6: step_select_model()
     elif st.session_state.current_step==7: step_input_single_question()
     elif st.session_state.current_step==8: step_show_single_answer()

if __name__=="__main__":
     main()

